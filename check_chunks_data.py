#!/usr/bin/env python3
"""
å…¨é¢æ£€æŸ¥chunksè¡¨æ•°æ®è´¨é‡

æ£€æŸ¥chunksè¡¨ä¸­æ˜¯å¦å­˜åœ¨æµ‹è¯•æ•°æ®æˆ–å…¶ä»–ä¸æ­£ç¡®çš„æ•°æ®
"""

import asyncio
import sys
import json
from pathlib import Path

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
project_root = Path(__file__).parent
sys.path.insert(0, str(project_root))
sys.path.insert(0, str(project_root / "src"))

# åŠ è½½ç¯å¢ƒå˜é‡
from dotenv import load_dotenv
load_dotenv(project_root / ".env")

from src.database import create_database_client
from src.utils.logger import setup_logger

logger = setup_logger(__name__)


async def check_chunks_data():
    """å…¨é¢æ£€æŸ¥chunksè¡¨æ•°æ®è´¨é‡"""
    logger.info("ğŸ” å¼€å§‹å…¨é¢æ£€æŸ¥chunksè¡¨æ•°æ®è´¨é‡...")
    
    # åˆå§‹åŒ–æ•°æ®åº“è¿æ¥
    db_client = create_database_client()
    await db_client.initialize()
    
    try:
        # æ£€æŸ¥1ï¼šåŸºæœ¬ç»Ÿè®¡ä¿¡æ¯
        logger.info("=" * 80)
        logger.info("æ£€æŸ¥1ï¼šåŸºæœ¬ç»Ÿè®¡ä¿¡æ¯")
        
        total_result = await db_client.fetch_one("SELECT COUNT(*) as count FROM chunks")
        total_chunks = total_result['count']
        logger.info(f"ğŸ“Š chunksè¡¨æ€»è®°å½•æ•°: {total_chunks}")
        
        # æ£€æŸ¥ä¸åŒURLçš„æ•°é‡
        url_result = await db_client.fetch_one("SELECT COUNT(DISTINCT url) as count FROM chunks")
        unique_urls = url_result['count']
        logger.info(f"ğŸ“Š å”¯ä¸€URLæ•°é‡: {unique_urls}")
        
        # å¹³å‡æ¯ä¸ªURLçš„chunksæ•°
        avg_chunks = total_chunks / unique_urls if unique_urls > 0 else 0
        logger.info(f"ğŸ“Š å¹³å‡æ¯ä¸ªURLçš„chunksæ•°: {avg_chunks:.2f}")
        
        # æ£€æŸ¥2ï¼šæŸ¥æ‰¾å¯ç–‘çš„æµ‹è¯•æ•°æ®
        logger.info("=" * 80)
        logger.info("æ£€æŸ¥2ï¼šæŸ¥æ‰¾å¯ç–‘çš„æµ‹è¯•æ•°æ®")
        
        # æŸ¥æ‰¾åŒ…å«"test"å…³é”®è¯çš„å†…å®¹
        test_results = await db_client.fetch_all("""
            SELECT id, url, content, LENGTH(content) as content_length
            FROM chunks
            WHERE LOWER(content) LIKE '%test%'
            ORDER BY id
            LIMIT 20
        """)
        
        logger.info(f"ğŸ“Š åŒ…å«'test'å…³é”®è¯çš„chunks: {len(test_results)}")
        
        if test_results:
            logger.warning("âš ï¸ å‘ç°å¯ç–‘çš„æµ‹è¯•æ•°æ®:")
            for i, row in enumerate(test_results[:10]):  # åªæ˜¾ç¤ºå‰10ä¸ª
                logger.warning(f"   {i+1}. ID: {row['id']}, é•¿åº¦: {row['content_length']}, URL: {row['url']}")
                content_preview = row['content'][:100].replace('\n', ' ')
                logger.warning(f"      é¢„è§ˆ: {content_preview}...")
        
        # æ£€æŸ¥3ï¼šæŸ¥æ‰¾å¼‚å¸¸çŸ­çš„å†…å®¹
        logger.info("=" * 80)
        logger.info("æ£€æŸ¥3ï¼šæŸ¥æ‰¾å¼‚å¸¸çŸ­çš„å†…å®¹")
        
        short_results = await db_client.fetch_all("""
            SELECT id, url, content, LENGTH(content) as content_length
            FROM chunks
            WHERE LENGTH(content) < 100
            ORDER BY LENGTH(content) ASC
            LIMIT 20
        """)
        
        logger.info(f"ğŸ“Š å†…å®¹é•¿åº¦å°äº100å­—ç¬¦çš„chunks: {len(short_results)}")
        
        if short_results:
            logger.warning("âš ï¸ å‘ç°å¼‚å¸¸çŸ­çš„å†…å®¹:")
            for i, row in enumerate(short_results[:10]):
                logger.warning(f"   {i+1}. ID: {row['id']}, é•¿åº¦: {row['content_length']}, URL: {row['url']}")
                content_preview = row['content'][:50].replace('\n', ' ')
                logger.warning(f"      å†…å®¹: {content_preview}...")
        
        # æ£€æŸ¥4ï¼šæŸ¥æ‰¾ç‰¹å®šçš„é—®é¢˜URL
        logger.info("=" * 80)
        logger.info("æ£€æŸ¥4ï¼šæŸ¥æ‰¾ç‰¹å®šçš„é—®é¢˜URL")
        
        swift_results = await db_client.fetch_all("""
            SELECT id, url, content, LENGTH(content) as content_length
            FROM chunks
            WHERE url = 'https://developer.apple.com/documentation/swift'
            ORDER BY id
        """)
        
        logger.info(f"ğŸ“Š Swiftæ–‡æ¡£URLçš„chunksæ•°é‡: {len(swift_results)}")
        
        if swift_results:
            logger.info("ğŸ“‹ Swiftæ–‡æ¡£çš„æ‰€æœ‰chunks:")
            for i, row in enumerate(swift_results):
                logger.info(f"   {i+1}. ID: {row['id']}, é•¿åº¦: {row['content_length']}")
                content_preview = row['content'][:100].replace('\n', ' ')
                logger.info(f"      é¢„è§ˆ: {content_preview}...")
                
                # æ£€æŸ¥æ˜¯å¦æ˜¯JSONæ ¼å¼çš„æµ‹è¯•æ•°æ®
                try:
                    parsed = json.loads(row['content'])
                    if isinstance(parsed, dict) and 'content' in parsed:
                        logger.error(f"âŒ å‘ç°JSONæ ¼å¼çš„æµ‹è¯•æ•°æ®: ID {row['id']}")
                        logger.error(f"   JSONå†…å®¹: {json.dumps(parsed, indent=2)}")
                except json.JSONDecodeError:
                    pass  # ä¸æ˜¯JSONï¼Œæ­£å¸¸
        
        # æ£€æŸ¥5ï¼šæŸ¥æ‰¾å…¶ä»–å¯ç–‘æ¨¡å¼
        logger.info("=" * 80)
        logger.info("æ£€æŸ¥5ï¼šæŸ¥æ‰¾å…¶ä»–å¯ç–‘æ¨¡å¼")
        
        # æŸ¥æ‰¾åŒ…å«JSONç»“æ„çš„å†…å®¹
        json_results = await db_client.fetch_all("""
            SELECT id, url, content, LENGTH(content) as content_length
            FROM chunks
            WHERE content LIKE '%{%}%' AND content LIKE '%"content"%'
            ORDER BY id
            LIMIT 10
        """)
        
        logger.info(f"ğŸ“Š å¯èƒ½åŒ…å«JSONç»“æ„çš„chunks: {len(json_results)}")
        
        if json_results:
            logger.warning("âš ï¸ å‘ç°å¯èƒ½çš„JSONç»“æ„æ•°æ®:")
            for i, row in enumerate(json_results):
                logger.warning(f"   {i+1}. ID: {row['id']}, é•¿åº¦: {row['content_length']}, URL: {row['url']}")
                content_preview = row['content'][:100].replace('\n', ' ')
                logger.warning(f"      é¢„è§ˆ: {content_preview}...")
        
        # æ£€æŸ¥6ï¼šURLåˆ†å¸ƒç»Ÿè®¡
        logger.info("=" * 80)
        logger.info("æ£€æŸ¥6ï¼šURLåˆ†å¸ƒç»Ÿè®¡")
        
        url_stats = await db_client.fetch_all("""
            SELECT url, COUNT(*) as chunk_count, 
                   MIN(LENGTH(content)) as min_length,
                   MAX(LENGTH(content)) as max_length,
                   AVG(LENGTH(content))::int as avg_length
            FROM chunks
            GROUP BY url
            ORDER BY chunk_count DESC
            LIMIT 20
        """)
        
        logger.info("ğŸ“Š URL chunksæ•°é‡æ’è¡Œæ¦œ (å‰20):")
        for i, row in enumerate(url_stats):
            logger.info(f"   {i+1}. {row['url'][:80]}...")
            logger.info(f"      chunksæ•°: {row['chunk_count']}, é•¿åº¦èŒƒå›´: {row['min_length']}-{row['max_length']}, å¹³å‡: {row['avg_length']}")
        
        # æ£€æŸ¥7ï¼šembeddingå­—æ®µçŠ¶æ€
        logger.info("=" * 80)
        logger.info("æ£€æŸ¥7ï¼šembeddingå­—æ®µçŠ¶æ€")
        
        embedding_stats = await db_client.fetch_one("""
            SELECT 
                COUNT(*) as total,
                COUNT(embedding) as with_embedding,
                COUNT(*) - COUNT(embedding) as without_embedding
            FROM chunks
        """)
        
        logger.info(f"ğŸ“Š embeddingå­—æ®µç»Ÿè®¡:")
        logger.info(f"   æ€»è®°å½•æ•°: {embedding_stats['total']}")
        logger.info(f"   æœ‰embedding: {embedding_stats['with_embedding']}")
        logger.info(f"   æ— embedding: {embedding_stats['without_embedding']}")
        
        if embedding_stats['without_embedding'] > 0:
            logger.warning(f"âš ï¸ å‘ç° {embedding_stats['without_embedding']} ä¸ªè®°å½•ç¼ºå°‘embedding")
        
    except Exception as e:
        logger.error(f"âŒ æ£€æŸ¥è¿‡ç¨‹ä¸­å‡ºé”™: {e}")
        
    finally:
        await db_client.close()
        logger.info("ğŸ”’ æ•°æ®åº“è¿æ¥å·²å…³é—­")


async def main():
    """ä¸»å‡½æ•°"""
    logger.info("ğŸš€ å¼€å§‹chunksè¡¨æ•°æ®è´¨é‡æ£€æŸ¥...")
    await check_chunks_data()
    logger.info("ğŸ‰ æ£€æŸ¥å®Œæˆï¼")


if __name__ == "__main__":
    asyncio.run(main())
