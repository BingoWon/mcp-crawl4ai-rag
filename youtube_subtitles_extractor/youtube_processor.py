#!/usr/bin/env python3
"""
YouTubeå­—å¹•å¤„ç†å™¨ - å®Œæ•´æµç¨‹å®ç°

åŠŸèƒ½ï¼š
1. ä»pagesè¡¨è¯»å–YouTubeå­—å¹•æ•°æ®
2. è¿›è¡Œchunkingåˆ†å—å¤„ç†
3. æ‰¹é‡embeddingå¤„ç†
4. å­˜å‚¨åˆ°chunksè¡¨
5. æ›´æ–°pagesè¡¨processed_atçŠ¶æ€

å¤„ç†å•ä½ï¼šä¸€ä¸ªè§†é¢‘çš„æ‰€æœ‰chunksä¸€èµ·å¤„ç†
"""

import asyncio
import json
import sys
from pathlib import Path
from typing import List, Dict, Any, Tuple
from datetime import datetime

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))
sys.path.insert(0, str(project_root / "src"))

# åŠ è½½ç¯å¢ƒå˜é‡
from dotenv import load_dotenv
load_dotenv(project_root / ".env")

from src.database import create_database_client
from src.database.operations import DatabaseOperations
from src.embedding import get_embedder
from src.embedding.providers import SiliconFlowProvider
from src.utils.logger import setup_logger
from youtube_chunker import YouTubeChunker

logger = setup_logger(__name__)


class YouTubeProcessor:
    """YouTubeå­—å¹•å®Œæ•´å¤„ç†å™¨"""
    
    def __init__(self):
        self.db_client = None
        self.chunker = YouTubeChunker()
        
    async def initialize(self):
        """åˆå§‹åŒ–æ•°æ®åº“è¿æ¥"""
        logger.info("ğŸ”— åˆå§‹åŒ–YouTubeå¤„ç†å™¨...")
        self.db_client = create_database_client()
        await self.db_client.initialize()
        logger.info("âœ… æ•°æ®åº“è¿æ¥æˆåŠŸ")
    
    async def get_unprocessed_youtube_videos(self, limit: int = 10) -> List[Tuple[str, Dict]]:
        """
        è·å–æœªå¤„ç†çš„YouTubeè§†é¢‘æ•°æ®
        
        Args:
            limit: è·å–æ•°é‡é™åˆ¶
            
        Returns:
            List of (url, video_data) tuples
        """
        logger.info(f"ğŸ“Š è·å–æœªå¤„ç†çš„YouTubeè§†é¢‘æ•°æ®ï¼Œé™åˆ¶: {limit}")
        
        query = """
            SELECT url, content 
            FROM pages 
            WHERE url LIKE 'https://www.youtube.com/watch?v=%' 
            AND processed_at IS NULL
            ORDER BY created_at ASC
            LIMIT $1
        """
        
        records = await self.db_client.fetch_all(query, limit)
        
        results = []
        for record in records:
            try:
                video_data = json.loads(record['content'])
                results.append((record['url'], video_data))
            except json.JSONDecodeError as e:
                logger.error(f"âŒ JSONè§£æå¤±è´¥ {record['url']}: {e}")
                continue
        
        logger.info(f"âœ… è·å–åˆ° {len(results)} ä¸ªæœ‰æ•ˆYouTubeè§†é¢‘")
        return results

    async def _batch_embedding(self, json_chunks: List[str]) -> List[List[float]]:
        """
        æ‰¹é‡embeddingå¤„ç† - ä½¿ç”¨SiliconFlow APIæ‰¹é‡æ¥å£

        Args:
            json_chunks: JSONå­—ç¬¦ä¸²åˆ—è¡¨

        Returns:
            embeddingåˆ—è¡¨ï¼Œå¤±è´¥çš„ä¸ºNone
        """
        embedder = get_embedder()

        # é¡¹ç›®è¦æ±‚ï¼šæ°¸è¿œä¸å…è®¸ä½¿ç”¨æœ¬åœ°æ¨¡å‹ï¼Œåªä½¿ç”¨API
        if not isinstance(embedder, SiliconFlowProvider):
            raise RuntimeError("Only SiliconFlow API embedding is allowed, local models are prohibited")

        try:
            # ä½¿ç”¨æ‰¹é‡APIè°ƒç”¨ - å•æ¬¡APIè¯·æ±‚å¤„ç†æ‰€æœ‰æ–‡æœ¬
            batch_embeddings = await embedder.encode_batch_concurrent(json_chunks)
            logger.info(f"âœ… æ‰¹é‡embeddingå®Œæˆ: {len(batch_embeddings)} ä¸ª")
            return batch_embeddings
        except Exception as e:
            logger.error(f"âŒ æ‰¹é‡embeddingå¤±è´¥: {e}")
            # è¿”å›Noneåˆ—è¡¨ï¼Œè®©ä¸Šå±‚å¤„ç†å¤±è´¥æƒ…å†µ
            return [None] * len(json_chunks)

    async def process_single_video(self, url: str, video_data: Dict) -> Dict[str, Any]:
        """
        å¤„ç†å•ä¸ªè§†é¢‘çš„å®Œæ•´æµç¨‹
        
        Args:
            url: YouTube URL
            video_data: è§†é¢‘æ•°æ® {"context": "æ ‡é¢˜", "content": "å­—å¹•"}
            
        Returns:
            å¤„ç†ç»“æœç»Ÿè®¡
        """
        logger.info(f"ğŸ¬ å¼€å§‹å¤„ç†è§†é¢‘: {video_data['context']}")
        
        try:
            # 1. Chunkingåˆ†å—
            chunks = self.chunker.chunk_youtube_subtitle(video_data)
            if not chunks:
                logger.warning(f"âš ï¸ è§†é¢‘æ— æœ‰æ•ˆchunks: {url}")
                return {"success": False, "error": "No valid chunks"}
            
            # 2. è½¬æ¢ä¸ºJSONå­—ç¬¦ä¸²
            json_chunks = self.chunker.chunk_to_json_strings(chunks)
            logger.info(f"ğŸ“¦ ç”Ÿæˆ {len(json_chunks)} ä¸ªchunks")
            
            # 3. æ‰¹é‡embeddingå¤„ç†
            logger.info("ğŸ§  å¼€å§‹embeddingå¤„ç†...")
            embeddings = await self._batch_embedding(json_chunks)
            
            # è¿‡æ»¤æˆåŠŸçš„embeddings
            valid_data = []
            for i, (json_chunk, embedding) in enumerate(zip(json_chunks, embeddings)):
                if embedding is not None:
                    valid_data.append({
                        "url": url,
                        "content": json_chunk,
                        "embedding": str(embedding)
                    })
                else:
                    logger.warning(f"âš ï¸ è·³è¿‡å¤±è´¥çš„chunk {i+1}")
            
            if not valid_data:
                logger.error(f"âŒ æ‰€æœ‰chunks embeddingå¤±è´¥: {url}")
                return {"success": False, "error": "All embeddings failed"}
            
            # 4. æ‰¹é‡å­˜å‚¨åˆ°chunksè¡¨
            logger.info(f"ğŸ’¾ å­˜å‚¨ {len(valid_data)} ä¸ªchunksåˆ°æ•°æ®åº“...")
            await self._store_chunks_batch(url, valid_data)
            
            # 5. æ›´æ–°pagesè¡¨processed_at
            await self._mark_video_processed(url)
            
            logger.info(f"âœ… è§†é¢‘å¤„ç†å®Œæˆ: {len(valid_data)} chunks")
            
            return {
                "success": True,
                "total_chunks": len(json_chunks),
                "valid_chunks": len(valid_data),
                "failed_chunks": len(json_chunks) - len(valid_data)
            }
            
        except Exception as e:
            logger.error(f"âŒ è§†é¢‘å¤„ç†å¤±è´¥ {url}: {e}")
            return {"success": False, "error": str(e)}
    
    async def _store_chunks_batch(self, url: str, chunk_data: List[Dict[str, Any]]):
        """
        æ‰¹é‡å­˜å‚¨chunksåˆ°æ•°æ®åº“
        
        Args:
            url: YouTube URL
            chunk_data: chunkæ•°æ®åˆ—è¡¨
        """
        # å…ˆåˆ é™¤è¯¥URLçš„ç°æœ‰chunksï¼ˆå¦‚æœæœ‰ï¼‰
        delete_query = "DELETE FROM chunks WHERE url = $1"
        await self.db_client.execute_command(delete_query, url)

        # æ‰¹é‡æ’å…¥æ–°chunks
        insert_query = """
            INSERT INTO chunks (url, content, embedding)
            VALUES ($1, $2, $3)
        """

        insert_data = [
            (item["url"], item["content"], item["embedding"])
            for item in chunk_data
        ]

        await self.db_client.execute_many(insert_query, insert_data)
        logger.info(f"ğŸ’¾ æˆåŠŸå­˜å‚¨ {len(insert_data)} ä¸ªchunks")
    
    async def _mark_video_processed(self, url: str):
        """
        æ ‡è®°è§†é¢‘ä¸ºå·²å¤„ç†
        
        Args:
            url: YouTube URL
        """
        update_query = """
            UPDATE pages
            SET processed_at = NOW()
            WHERE url = $1
        """

        await self.db_client.execute_command(update_query, url)
        logger.info(f"âœ… æ ‡è®°è§†é¢‘å·²å¤„ç†: {url}")
    
    async def process_batch(self, batch_size: int = 5) -> Dict[str, Any]:
        """
        æ‰¹é‡å¤„ç†YouTubeè§†é¢‘
        
        Args:
            batch_size: æ‰¹å¤„ç†å¤§å°
            
        Returns:
            å¤„ç†ç»“æœç»Ÿè®¡
        """
        logger.info(f"ğŸš€ å¼€å§‹æ‰¹é‡å¤„ç†YouTubeè§†é¢‘ï¼Œæ‰¹å¤§å°: {batch_size}")
        
        # è·å–æœªå¤„ç†çš„è§†é¢‘
        videos = await self.get_unprocessed_youtube_videos(batch_size)
        
        if not videos:
            logger.info("â„¹ï¸ æ²¡æœ‰æœªå¤„ç†çš„YouTubeè§†é¢‘")
            return {"total_videos": 0, "processed": 0, "failed": 0}
        
        # å¤„ç†ç»Ÿè®¡
        total_videos = len(videos)
        processed_count = 0
        failed_count = 0
        total_chunks = 0
        
        logger.info(f"ğŸ“Š å¼€å§‹å¤„ç† {total_videos} ä¸ªè§†é¢‘")
        
        for i, (url, video_data) in enumerate(videos, 1):
            logger.info(f"ğŸ“¹ å¤„ç†è¿›åº¦: {i}/{total_videos}")
            
            result = await self.process_single_video(url, video_data)
            
            if result["success"]:
                processed_count += 1
                total_chunks += result["valid_chunks"]
                logger.info(f"âœ… è§†é¢‘ {i} å¤„ç†æˆåŠŸ: {result['valid_chunks']} chunks")
            else:
                failed_count += 1
                logger.error(f"âŒ è§†é¢‘ {i} å¤„ç†å¤±è´¥: {result.get('error', 'Unknown error')}")
        
        # æœ€ç»ˆç»Ÿè®¡
        stats = {
            "total_videos": total_videos,
            "processed": processed_count,
            "failed": failed_count,
            "total_chunks": total_chunks,
            "success_rate": processed_count / total_videos * 100 if total_videos > 0 else 0
        }
        
        logger.info("=" * 60)
        logger.info("ğŸ“Š æ‰¹é‡å¤„ç†å®Œæˆç»Ÿè®¡:")
        logger.info(f"   æ€»è§†é¢‘æ•°: {stats['total_videos']}")
        logger.info(f"   æˆåŠŸå¤„ç†: {stats['processed']}")
        logger.info(f"   å¤„ç†å¤±è´¥: {stats['failed']}")
        logger.info(f"   æ€»chunks: {stats['total_chunks']}")
        logger.info(f"   æˆåŠŸç‡: {stats['success_rate']:.1f}%")
        
        return stats
    
    async def cleanup(self):
        """æ¸…ç†èµ„æº"""
        if self.db_client:
            await self.db_client.close()
            logger.info("ğŸ”’ æ•°æ®åº“è¿æ¥å·²å…³é—­")


async def main():
    """ä¸»å‡½æ•°"""
    processor = YouTubeProcessor()
    
    try:
        await processor.initialize()
        
        # å¤„ç†1ä¸ªè§†é¢‘ä½œä¸ºæµ‹è¯•
        result = await processor.process_batch(batch_size=1)
        
        if result["processed"] > 0:
            logger.info("ğŸ‰ YouTubeå­—å¹•å¤„ç†å®Œæˆï¼")
        else:
            logger.warning("âš ï¸ æ²¡æœ‰æˆåŠŸå¤„ç†ä»»ä½•è§†é¢‘")
            
    except Exception as e:
        logger.error(f"âŒ å¤„ç†è¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {e}")
        
    finally:
        await processor.cleanup()


if __name__ == "__main__":
    asyncio.run(main())
