#!/usr/bin/env python3
"""
YouTubeå­—å¹•æ•°æ®å¯¼å…¥æ•°æ®åº“è„šæœ¬

åŠŸèƒ½ï¼š
- è¯»å–æ‰€æœ‰YouTubeå­—å¹•JSONæ–‡ä»¶
- è½¬æ¢ä¸ºpagesè¡¨æ ¼å¼
- æ‰¹é‡å¯¼å…¥åˆ°PostgreSQLæ•°æ®åº“

æ•°æ®æ ¼å¼ï¼š
- URL: https://www.youtube.com/watch?v={video_id}
- content: å®Œæ•´JSONå­—ç¬¦ä¸²
- created_at: NOW()
- processed_at: NULL
"""

import asyncio
import json
import sys
from pathlib import Path
from typing import List, Dict, Tuple
from datetime import datetime

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))
sys.path.insert(0, str(project_root / "src"))

# åŠ è½½ç¯å¢ƒå˜é‡
from dotenv import load_dotenv
load_dotenv(project_root / ".env")

from src.database import create_database_client
from src.utils.logger import setup_logger

logger = setup_logger(__name__)


class YouTubeDataImporter:
    """YouTubeå­—å¹•æ•°æ®å¯¼å…¥å™¨"""
    
    def __init__(self, subtitles_dir: str = "subtitles"):
        self.subtitles_dir = Path(subtitles_dir)
        self.db_client = None
        
    async def initialize(self):
        """åˆå§‹åŒ–æ•°æ®åº“è¿æ¥"""
        logger.info("ğŸ”— åˆå§‹åŒ–æ•°æ®åº“è¿æ¥...")
        self.db_client = create_database_client()
        await self.db_client.initialize()
        logger.info("âœ… æ•°æ®åº“è¿æ¥æˆåŠŸ")
        
    async def load_json_files(self) -> List[Tuple[str, Dict]]:
        """åŠ è½½æ‰€æœ‰JSONæ–‡ä»¶"""
        logger.info(f"ğŸ“‚ æ‰«æå­—å¹•ç›®å½•: {self.subtitles_dir}")
        
        json_files = list(self.subtitles_dir.glob("*.json"))
        logger.info(f"ğŸ“Š æ‰¾åˆ° {len(json_files)} ä¸ªJSONæ–‡ä»¶")
        
        data_list = []
        failed_files = []
        
        for json_file in json_files:
            try:
                # ä»æ–‡ä»¶åæå–video_id
                video_id = json_file.stem
                
                # è¯»å–JSONå†…å®¹
                with open(json_file, 'r', encoding='utf-8') as f:
                    json_data = json.load(f)
                
                data_list.append((video_id, json_data))
                
            except Exception as e:
                logger.error(f"âŒ è¯»å–æ–‡ä»¶å¤±è´¥ {json_file}: {e}")
                failed_files.append(str(json_file))
        
        logger.info(f"âœ… æˆåŠŸåŠ è½½ {len(data_list)} ä¸ªæ–‡ä»¶")
        if failed_files:
            logger.warning(f"âš ï¸ å¤±è´¥æ–‡ä»¶æ•°: {len(failed_files)}")
            
        return data_list
    
    def prepare_database_records(self, data_list: List[Tuple[str, Dict]]) -> List[Tuple[str, str]]:
        """å‡†å¤‡æ•°æ®åº“æ’å…¥è®°å½•"""
        logger.info("ğŸ”„ å‡†å¤‡æ•°æ®åº“è®°å½•...")
        
        records = []
        for video_id, json_data in data_list:
            # æ„é€ YouTube URL
            url = f"https://www.youtube.com/watch?v={video_id}"
            
            # å°†JSONæ•°æ®è½¬æ¢ä¸ºå­—ç¬¦ä¸²
            content = json.dumps(json_data, ensure_ascii=False, indent=2)
            
            records.append((url, content))
        
        logger.info(f"âœ… å‡†å¤‡äº† {len(records)} æ¡è®°å½•")
        return records
    
    async def check_existing_urls(self, urls: List[str]) -> List[str]:
        """æ£€æŸ¥å·²å­˜åœ¨çš„URL"""
        logger.info("ğŸ” æ£€æŸ¥æ•°æ®åº“ä¸­å·²å­˜åœ¨çš„URL...")
        
        if not urls:
            return []
        
        # æŸ¥è¯¢å·²å­˜åœ¨çš„URL
        placeholders = ','.join([f'${i+1}' for i in range(len(urls))])
        query = f"""
            SELECT url FROM pages 
            WHERE url IN ({placeholders})
        """
        
        existing_records = await self.db_client.fetch_all(query, *urls)
        existing_urls = [record['url'] for record in existing_records]
        
        logger.info(f"ğŸ“Š å·²å­˜åœ¨URLæ•°é‡: {len(existing_urls)}")
        return existing_urls
    
    async def insert_records(self, records: List[Tuple[str, str]], 
                           handle_duplicates: str = "skip") -> Dict[str, int]:
        """æ’å…¥è®°å½•åˆ°æ•°æ®åº“"""
        logger.info(f"ğŸ’¾ å¼€å§‹æ’å…¥ {len(records)} æ¡è®°å½•...")
        logger.info(f"ğŸ”§ é‡å¤å¤„ç†ç­–ç•¥: {handle_duplicates}")
        
        urls = [record[0] for record in records]
        existing_urls = await self.check_existing_urls(urls)
        
        stats = {
            "total": len(records),
            "existing": len(existing_urls),
            "inserted": 0,
            "updated": 0,
            "failed": 0
        }
        
        if handle_duplicates == "skip":
            # è·³è¿‡å·²å­˜åœ¨çš„è®°å½•
            new_records = [
                record for record in records 
                if record[0] not in existing_urls
            ]
            
            if new_records:
                try:
                    await self.db_client.execute_many("""
                        INSERT INTO pages (url, content, created_at, processed_at)
                        VALUES ($1, $2, NOW(), NULL)
                    """, new_records)
                    
                    stats["inserted"] = len(new_records)
                    logger.info(f"âœ… æˆåŠŸæ’å…¥ {len(new_records)} æ¡æ–°è®°å½•")
                    
                except Exception as e:
                    logger.error(f"âŒ æ’å…¥å¤±è´¥: {e}")
                    stats["failed"] = len(new_records)
            else:
                logger.info("â„¹ï¸ æ‰€æœ‰è®°å½•éƒ½å·²å­˜åœ¨ï¼Œè·³è¿‡æ’å…¥")
                
        elif handle_duplicates == "update":
            # æ›´æ–°å·²å­˜åœ¨çš„è®°å½•
            existing_records = [
                record for record in records 
                if record[0] in existing_urls
            ]
            new_records = [
                record for record in records 
                if record[0] not in existing_urls
            ]
            
            # æ’å…¥æ–°è®°å½•
            if new_records:
                try:
                    await self.db_client.execute_many("""
                        INSERT INTO pages (url, content, created_at, processed_at)
                        VALUES ($1, $2, NOW(), NULL)
                    """, new_records)
                    stats["inserted"] = len(new_records)
                    logger.info(f"âœ… æˆåŠŸæ’å…¥ {len(new_records)} æ¡æ–°è®°å½•")
                except Exception as e:
                    logger.error(f"âŒ æ’å…¥æ–°è®°å½•å¤±è´¥: {e}")
                    stats["failed"] += len(new_records)
            
            # æ›´æ–°å·²å­˜åœ¨çš„è®°å½•
            if existing_records:
                try:
                    await self.db_client.execute_many("""
                        UPDATE pages 
                        SET content = $2, created_at = NOW(), processed_at = NULL
                        WHERE url = $1
                    """, existing_records)
                    stats["updated"] = len(existing_records)
                    logger.info(f"âœ… æˆåŠŸæ›´æ–° {len(existing_records)} æ¡å·²å­˜åœ¨è®°å½•")
                except Exception as e:
                    logger.error(f"âŒ æ›´æ–°è®°å½•å¤±è´¥: {e}")
                    stats["failed"] += len(existing_records)
        
        return stats
    
    async def verify_import(self, sample_urls: List[str] = None) -> Dict[str, any]:
        """éªŒè¯å¯¼å…¥ç»“æœ"""
        logger.info("ğŸ” éªŒè¯å¯¼å…¥ç»“æœ...")
        
        # ç»Ÿè®¡YouTube URLæ•°é‡
        youtube_count = await self.db_client.fetch_val("""
            SELECT COUNT(*) FROM pages 
            WHERE url LIKE 'https://www.youtube.com/watch?v=%'
        """)
        
        # è·å–æ ·æœ¬æ•°æ®
        sample_data = await self.db_client.fetch_all("""
            SELECT url, LENGTH(content) as content_length, created_at, processed_at
            FROM pages 
            WHERE url LIKE 'https://www.youtube.com/watch?v=%'
            ORDER BY created_at DESC
            LIMIT 5
        """)
        
        verification = {
            "youtube_urls_count": youtube_count,
            "sample_data": sample_data
        }
        
        logger.info(f"ğŸ“Š YouTube URLæ€»æ•°: {youtube_count}")
        logger.info("ğŸ“ æ ·æœ¬æ•°æ®:")
        for sample in sample_data:
            logger.info(f"  - {sample['url']}: {sample['content_length']}å­—ç¬¦, {sample['created_at']}")
        
        return verification
    
    async def run_import(self, handle_duplicates: str = "skip") -> Dict[str, any]:
        """è¿è¡Œå®Œæ•´å¯¼å…¥æµç¨‹"""
        try:
            await self.initialize()
            
            # åŠ è½½JSONæ–‡ä»¶
            data_list = await self.load_json_files()
            if not data_list:
                logger.error("âŒ æ²¡æœ‰æ‰¾åˆ°æœ‰æ•ˆçš„JSONæ–‡ä»¶")
                return {"success": False, "error": "No valid JSON files found"}
            
            # å‡†å¤‡æ•°æ®åº“è®°å½•
            records = self.prepare_database_records(data_list)
            
            # æ’å…¥è®°å½•
            stats = await self.insert_records(records, handle_duplicates)
            
            # éªŒè¯ç»“æœ
            verification = await self.verify_import()
            
            logger.info("ğŸ‰ å¯¼å…¥å®Œæˆï¼")
            
            return {
                "success": True,
                "stats": stats,
                "verification": verification
            }
            
        except Exception as e:
            logger.error(f"âŒ å¯¼å…¥è¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {e}")
            return {"success": False, "error": str(e)}
            
        finally:
            if self.db_client:
                await self.db_client.close()


async def main():
    """ä¸»å‡½æ•°"""
    import argparse
    
    parser = argparse.ArgumentParser(description="YouTubeå­—å¹•æ•°æ®å¯¼å…¥æ•°æ®åº“")
    parser.add_argument("--duplicates", choices=["skip", "update"], default="skip",
                       help="é‡å¤URLå¤„ç†ç­–ç•¥: skip(è·³è¿‡) æˆ– update(æ›´æ–°)")
    parser.add_argument("--subtitles-dir", default="subtitles",
                       help="å­—å¹•æ–‡ä»¶ç›®å½•")
    
    args = parser.parse_args()
    
    logger.info("ğŸš€ å¼€å§‹YouTubeå­—å¹•æ•°æ®å¯¼å…¥...")
    logger.info(f"ğŸ“‚ å­—å¹•ç›®å½•: {args.subtitles_dir}")
    logger.info(f"ğŸ”§ é‡å¤å¤„ç†: {args.duplicates}")
    
    importer = YouTubeDataImporter(args.subtitles_dir)
    result = await importer.run_import(args.duplicates)
    
    if result["success"]:
        stats = result["stats"]
        logger.info("=" * 50)
        logger.info("ğŸ“Š å¯¼å…¥ç»Ÿè®¡:")
        logger.info(f"   æ€»è®°å½•æ•°: {stats['total']}")
        logger.info(f"   å·²å­˜åœ¨: {stats['existing']}")
        logger.info(f"   æ–°æ’å…¥: {stats['inserted']}")
        logger.info(f"   æ›´æ–°: {stats['updated']}")
        logger.info(f"   å¤±è´¥: {stats['failed']}")
        logger.info(f"   æˆåŠŸç‡: {((stats['inserted'] + stats['updated']) / stats['total'] * 100):.1f}%")
        
        verification = result["verification"]
        logger.info(f"ğŸ“ˆ æ•°æ®åº“ä¸­YouTube URLæ€»æ•°: {verification['youtube_urls_count']}")
        
    else:
        logger.error(f"âŒ å¯¼å…¥å¤±è´¥: {result['error']}")
        sys.exit(1)


if __name__ == "__main__":
    asyncio.run(main())
