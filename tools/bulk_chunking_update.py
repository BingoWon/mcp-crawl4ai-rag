#!/usr/bin/env python3
"""
æ‰¹é‡Chunkingæ›´æ–°å·¥å…·

åŠŸèƒ½ï¼š
- å¯¹å…¨éƒ¨pagesè¡¨è¿›è¡Œchunkingæ›´æ–°å¤„ç†
- ä½¿ç”¨åŒé‡chunkingå¯¹æ¯”æœºåˆ¶ï¼ˆæ—§æ–¹æ¡ˆ vs æ–°æ–¹æ¡ˆï¼‰
- åªæœ‰ç»“æœä¸ä¸€è‡´æ—¶æ‰åˆ é™¤chunksè¡¨æ—§æ•°æ®å¹¶é‡æ–°å­˜å‚¨
- ç»“æœä¸€è‡´æ—¶å®Œå…¨è·³è¿‡ï¼Œä¸æ“ä½œchunksè¡¨

æ ¸å¿ƒé€»è¾‘ï¼š
1. è·å–æ‰€æœ‰pagesè¡¨è®°å½•
2. å¯¹æ¯ä¸ªpageè¿›è¡ŒåŒé‡chunking
3. å¯¹æ¯”ç»“æœæ˜¯å¦ä¸€è‡´
4. åªæœ‰ä¸ä¸€è‡´æ—¶æ‰æ›´æ–°chunksè¡¨
5. æ˜¾ç¤ºè¯¦ç»†çš„å¤„ç†è¿›åº¦å’Œç»Ÿè®¡ä¿¡æ¯
"""

import asyncio
import os
import sys
import time
from pathlib import Path
from typing import List, Dict, Any, Tuple

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))
sys.path.insert(0, str(project_root / "src"))

# åŠ è½½ç¯å¢ƒå˜é‡
from dotenv import load_dotenv
load_dotenv(project_root / ".env")

from src.database import create_database_client, DatabaseOperations
from src.chunking import SmartChunker
from src.chunking_deprecated.chunker import SmartChunker as DeprecatedChunker
from src.embedding import get_embedder
from src.utils.logger import setup_logger

logger = setup_logger(__name__)


class BulkChunkingUpdater:
    """æ‰¹é‡Chunkingæ›´æ–°å™¨"""

    def __init__(self):
        # è¶…å‚æ•°é…ç½®
        self.batch_size = int(os.getenv("BULK_UPDATE_BATCH_SIZE", "500"))

        # æ ¸å¿ƒç»„ä»¶
        self.db_client = None
        self.db_operations = None
        self.current_chunker = SmartChunker()  # æ–°æ–¹æ¡ˆ (2500/3000)
        self.deprecated_chunker = DeprecatedChunker()  # æ—§æ–¹æ¡ˆ (5000/6000)

        # ç»Ÿè®¡ä¿¡æ¯
        self.stats = {
            "total_pages": 0,
            "processed_pages": 0,
            "identical_results": 0,
            "different_results": 0,
            "chunks_updated": 0,
            "chunks_skipped": 0,
            "errors": 0,
            "start_time": None,
            "end_time": None
        }

    async def __aenter__(self):
        await self.initialize()
        return self

    async def __aexit__(self, _exc_type, _exc_val, _exc_tb):
        await self.cleanup()

    async def initialize(self):
        """åˆå§‹åŒ–æ•°æ®åº“è¿æ¥"""
        logger.info(f"ğŸ”— åˆå§‹åŒ–æ‰¹é‡Chunkingæ›´æ–°å™¨ | æ‰¹æ¬¡å¤§å°: {self.batch_size}")
        self.db_client = create_database_client()
        await self.db_client.initialize()
        self.db_operations = DatabaseOperations(self.db_client)
        logger.info("âœ… æ•°æ®åº“è¿æ¥æˆåŠŸ")

    async def cleanup(self):
        """æ¸…ç†èµ„æº"""
        if self.db_client:
            await self.db_client.close()
            logger.info("ğŸ”’ æ•°æ®åº“è¿æ¥å·²å…³é—­")

    def _compare_chunking_results(self, old_chunks: List[str], new_chunks: List[str]) -> bool:
        """å¯¹æ¯”ä¸¤ä¸ªchunkingç»“æœæ˜¯å¦ä¸€è‡´"""
        # 1. æ•°é‡å¯¹æ¯”
        if len(old_chunks) != len(new_chunks):
            return False
        
        # 2. å†…å®¹å¯¹æ¯”ï¼šé€ä¸ªæ¯”è¾ƒchunkå†…å®¹
        for old_chunk, new_chunk in zip(old_chunks, new_chunks):
            if old_chunk != new_chunk:
                return False
        
        return True

    async def get_total_apple_pages_count(self) -> int:
        """è·å–Appleæ–‡æ¡£æ€»æ•°ï¼ˆç”¨äºè¿›åº¦è®¡ç®—ï¼‰"""
        result = await self.db_client.fetch_one("""
            SELECT COUNT(*) as count FROM pages
            WHERE content IS NOT NULL
            AND content != ''
            AND (url = 'https://developer.apple.com/documentation'
                 OR url LIKE 'https://developer.apple.com/documentation/%')
        """)
        return result['count']

    async def process_single_page(self, url: str, content: str) -> Dict[str, Any]:
        """å¤„ç†å•ä¸ªé¡µé¢çš„chunkingæ›´æ–°"""
        try:
            # åŒé‡chunking
            old_chunks = self.deprecated_chunker.chunk_text(content)
            new_chunks = self.current_chunker.chunk_text(content)
            
            # æ™ºèƒ½å¯¹æ¯”
            is_identical = self._compare_chunking_results(old_chunks, new_chunks)
            
            result = {
                "url": url,
                "old_chunk_count": len(old_chunks),
                "new_chunk_count": len(new_chunks),
                "is_identical": is_identical,
                "chunks_processed": 0,
                "error": None
            }
            
            if is_identical:
                # ç»“æœä¸€è‡´ï¼Œè·³è¿‡chunksè¡¨æ“ä½œ
                self.stats["identical_results"] += 1
                self.stats["chunks_skipped"] += len(new_chunks)
                logger.debug(f"âœ… {url}: ç»“æœä¸€è‡´ï¼Œè·³è¿‡æ›´æ–°")
            else:
                # ç»“æœä¸ä¸€è‡´ï¼Œéœ€è¦æ›´æ–°chunksè¡¨
                self.stats["different_results"] += 1
                
                # åˆ é™¤æ—§chunks
                await self.db_operations.delete_chunks_batch([url])
                
                # ç”Ÿæˆæ–°chunksçš„embeddingå¹¶å­˜å‚¨
                chunks_data = await self._process_chunks_with_embedding(url, new_chunks)
                
                if chunks_data:
                    await self.db_operations.insert_chunks(chunks_data)
                    result["chunks_processed"] = len(chunks_data)
                    self.stats["chunks_updated"] += len(chunks_data)
                    logger.debug(f"ğŸ”„ {url}: æ›´æ–°äº† {len(chunks_data)} ä¸ªchunks")
                
                # æ³¨æ„ï¼šä¸åœ¨è¿™é‡Œæ ‡è®°ä¸ºå·²å¤„ç†ï¼Œè€Œæ˜¯åœ¨æ‰¹æ¬¡çº§åˆ«ç»Ÿä¸€æ ‡è®°
            
            return result
            
        except Exception as e:
            self.stats["errors"] += 1
            error_msg = f"å¤„ç†é¡µé¢å¤±è´¥: {e}"
            logger.error(f"âŒ {url}: {error_msg}")
            return {
                "url": url,
                "old_chunk_count": 0,
                "new_chunk_count": 0,
                "is_identical": False,
                "chunks_processed": 0,
                "error": error_msg
            }

    async def _process_chunks_with_embedding(self, url: str, chunks: List[str]) -> List[Dict[str, Any]]:
        """ä¸ºchunksç”Ÿæˆembeddingå¹¶å‡†å¤‡å­˜å‚¨æ•°æ®"""
        valid_chunks = [chunk for chunk in chunks if chunk.strip()]

        if not valid_chunks:
            return []

        # ç”Ÿæˆembeddingsï¼ˆåªä½¿ç”¨APIï¼‰
        embeddings = await self._generate_embeddings(valid_chunks)

        # å‡†å¤‡å­˜å‚¨æ•°æ®
        chunks_data = []
        for i, embedding in enumerate(embeddings):
            if embedding is not None:  # æˆåŠŸçš„embedding
                chunks_data.append({
                    "url": url,
                    "content": valid_chunks[i],
                    "embedding": str(embedding)
                })

        return chunks_data

    async def _generate_embeddings(self, chunks: List[str]) -> List[Any]:
        """ç”Ÿæˆembeddingsï¼ˆåªä½¿ç”¨APIæ‰¹é‡å¤„ç†ï¼‰"""
        embedder = get_embedder()

        # åªä½¿ç”¨API embeddingï¼Œæ‰¹é‡å¤„ç†
        try:
            return await embedder.encode_batch_concurrent(chunks)
        except Exception as e:
            logger.warning(f"æ‰¹é‡embeddingå¤±è´¥ï¼Œæ”¹ä¸ºé€ä¸ªå¤„ç†: {e}")
            # é™çº§ä¸ºé€ä¸ªå¤„ç†
            embeddings = []
            for chunk in chunks:
                try:
                    embedding = await embedder.encode_batch_concurrent([chunk])
                    embeddings.append(embedding[0] if embedding else None)
                except Exception as chunk_e:
                    logger.error(f"å•ä¸ªchunk embeddingå¤±è´¥: {chunk_e}")
                    embeddings.append(None)
            return embeddings

    async def run_bulk_update(self):
        """æ‰§è¡Œæ‰¹é‡æ›´æ–°"""
        logger.info("ğŸš€ å¼€å§‹æ‰¹é‡Chunkingæ›´æ–°...")
        self.stats["start_time"] = time.time()

        # è·å–Appleæ–‡æ¡£æ€»æ•°ï¼ˆç”¨äºè¿›åº¦è®¡ç®—ï¼‰
        total_count = await self.get_total_apple_pages_count()
        self.stats["total_pages"] = total_count

        if total_count == 0:
            logger.warning("âš ï¸ æ²¡æœ‰æ‰¾åˆ°Appleæ–‡æ¡£")
            return

        logger.info(f"ğŸ“Š å¼€å§‹åˆ†æ‰¹å¤„ç†ï¼Œæ€»è®¡ {total_count} ä¸ªAppleæ–‡æ¡£ï¼Œæ‰¹æ¬¡å¤§å°: {self.batch_size}")
        logger.info("ğŸ’¡ æ³¨æ„ï¼šåªæœ‰chunkingç»“æœä¸ä¸€è‡´çš„é¡µé¢æ‰ä¼šè¢«å¤„ç†")
        logger.info("=" * 80)

        # åˆ†æ‰¹å¤„ç†ï¼šä½¿ç”¨ç°æœ‰çš„get_process_urls_batch()æ–¹æ³•ï¼ˆæ–¹æ¡ˆBï¼‰
        processed_count = 0
        while True:
            # è·å–ä¸€æ‰¹å¾…å¤„ç†çš„é¡µé¢
            batch = await self.db_operations.get_process_urls_batch(self.batch_size)
            if not batch:
                break  # æ²¡æœ‰æ›´å¤šæ•°æ®

            batch_results = []
            batch_urls = [url for url, _ in batch]

            try:
                # å¹¶å‘å¤„ç†å½“å‰æ‰¹æ¬¡
                tasks = [self.process_single_page(url, content) for url, content in batch]
                batch_results = await asyncio.gather(*tasks, return_exceptions=True)

                # æ›´æ–°ç»Ÿè®¡
                for result in batch_results:
                    if isinstance(result, Exception):
                        self.stats["errors"] += 1
                        logger.error(f"âŒ æ‰¹æ¬¡å¤„ç†å¼‚å¸¸: {result}")
                    else:
                        self.stats["processed_pages"] += 1

                # ğŸ”¥ å…³é”®ä¿®å¤ï¼šæ‰¹æ¬¡çº§åˆ«æ ‡è®°ä¸ºå·²å¤„ç†ï¼ˆæ— è®ºæ˜¯å¦æœ‰é”™è¯¯ï¼‰
                await self.db_operations.mark_pages_processed(batch_urls)

            except Exception as e:
                # æ‰¹æ¬¡çº§åˆ«çš„é”™è¯¯å¤„ç†
                self.stats["errors"] += len(batch)
                logger.error(f"âŒ æ•´ä¸ªæ‰¹æ¬¡å¤„ç†å¤±è´¥: {e}")

                # å³ä½¿æ‰¹æ¬¡å¤±è´¥ï¼Œä¹Ÿè¦æ ‡è®°ä¸ºå·²å¤„ç†ï¼Œé¿å…é‡å¤å¤„ç†
                await self.db_operations.mark_pages_processed(batch_urls)

            processed_count += len(batch)

            # æ˜¾ç¤ºè¿›åº¦å’Œç»Ÿè®¡ï¼ˆæ•´åˆä¸ºä¸€è¡Œï¼‰
            progress = (processed_count / total_count * 100) if total_count > 0 else 0
            processed = self.stats["processed_pages"]
            identical = self.stats["identical_results"]
            identical_pct = (identical / processed * 100) if processed > 0 else 0
            logger.info(f"ğŸ“ˆ è¿›åº¦: {progress:.1f}% ({processed_count}/{total_count}) | ä¸€è‡´: {identical} ({identical_pct:.1f}%) | é”™è¯¯: {self.stats['errors']}")
        
        self.stats["end_time"] = time.time()
        self._log_final_stats()

    def _log_final_stats(self):
        """æ˜¾ç¤ºæœ€ç»ˆç»Ÿè®¡ä¿¡æ¯"""
        duration = self.stats["end_time"] - self.stats["start_time"]
        total = self.stats["total_pages"]
        processed = self.stats["processed_pages"]
        identical = self.stats["identical_results"]
        different = self.stats["different_results"]
        updated = self.stats["chunks_updated"]
        skipped = self.stats["chunks_skipped"]
        errors = self.stats["errors"]
        
        logger.info("=" * 80)
        logger.info("ğŸ¯ æ‰¹é‡Chunkingæ›´æ–°å®Œæˆï¼")
        logger.info("=" * 80)
        logger.info(f"â±ï¸  æ€»è€—æ—¶: {duration:.2f} ç§’")
        logger.info(f"ğŸ“„ æ€»é¡µé¢æ•°: {total}")
        logger.info(f"âœ… æˆåŠŸå¤„ç†: {processed}")
        logger.info(f"âŒ å¤„ç†å¤±è´¥: {errors}")
        logger.info("")
        logger.info("ğŸ“Š Chunkingå¯¹æ¯”ç»“æœ:")
        logger.info(f"   ğŸŸ¢ ç»“æœä¸€è‡´: {identical} ({(identical/processed*100):.1f}%)")
        logger.info(f"   ğŸŸ¡ ç»“æœä¸åŒ: {different} ({(different/processed*100):.1f}%)")
        logger.info("")
        logger.info("ğŸ’¾ Chunksè¡¨æ“ä½œ:")
        logger.info(f"   ğŸ”„ æ›´æ–°chunks: {updated}")
        logger.info(f"   â­ï¸  è·³è¿‡chunks: {skipped}")
        logger.info(f"   ğŸ’° èŠ‚çœembedding: {skipped} ({(skipped/(updated+skipped)*100):.1f}%)")
        logger.info("=" * 80)


async def main():
    """ä¸»å‡½æ•°"""
    logger.info("ğŸ¯ æ‰¹é‡Chunkingæ›´æ–°å·¥å…·")
    logger.info("åŠŸèƒ½: å¯¹å…¨éƒ¨pagesè¡¨è¿›è¡Œæ™ºèƒ½chunkingæ›´æ–°")
    logger.info("ç­–ç•¥: åªæœ‰ç»“æœä¸ä¸€è‡´æ—¶æ‰æ›´æ–°chunksè¡¨")
    logger.info("")
    
    async with BulkChunkingUpdater() as updater:
        await updater.run_bulk_update()
    
    logger.info("ğŸ‰ æ‰¹é‡æ›´æ–°ä»»åŠ¡å®Œæˆï¼")


if __name__ == "__main__":
    asyncio.run(main())
