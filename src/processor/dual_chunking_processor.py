"""
Dual Chunking Processor - åŒé‡åˆ†å—å¯¹æ¯”å¤„ç†å™¨

æ ¸å¿ƒç‰¹æ€§ï¼š
- åŒé‡chunkingï¼šåŒæ—¶ä½¿ç”¨æ—§æ–¹æ¡ˆå’Œæ–°æ–¹æ¡ˆè¿›è¡Œåˆ†å—
- æ™ºèƒ½å¯¹æ¯”ï¼šåªæœ‰ç»“æœä¸ä¸€è‡´æ—¶æ‰è¿›è¡Œembeddingå’Œå­˜å‚¨
- å±€éƒ¨æ›´æ–°ï¼šå¤§å¹…èŠ‚çœAPIè°ƒç”¨å’Œè®¡ç®—èµ„æº
- å…¼å®¹æ¶æ„ï¼šåŸºäºåŸprocessorçš„æˆç†Ÿæ¶æ„

å¯¹æ¯”é€»è¾‘ï¼š
1. æ•°é‡å¯¹æ¯”ï¼šchunkæ•°é‡æ˜¯å¦ç›¸åŒ
2. å†…å®¹å¯¹æ¯”ï¼šé€ä¸ªæ¯”è¾ƒchunkå†…å®¹æ˜¯å¦å®Œå…¨ä¸€è‡´
3. æ¡ä»¶å¤„ç†ï¼šåªæœ‰ä¸ä¸€è‡´æ—¶æ‰è¿›è¡Œåç»­å¤„ç†
"""

import sys
import os
import asyncio
import time
from pathlib import Path
from typing import List, Dict, Any, Tuple
sys.path.insert(0, str(Path(__file__).parent.parent))

from database import create_database_client, DatabaseOperations
from chunking import SmartChunker
from chunking_deprecated.chunker import SmartChunker as DeprecatedChunker
from embedding import create_embedding, get_embedder
from embedding.providers import SiliconFlowProvider
from utils.logger import setup_logger

logger = setup_logger(__name__)


class DualChunkingProcessor:
    """åŒé‡åˆ†å—å¯¹æ¯”å¤„ç†å™¨ - æ™ºèƒ½å±€éƒ¨æ›´æ–°"""

    # ç³»ç»Ÿå¸¸é‡
    BUFFER_CHECK_INTERVAL = 1.0
    NO_CONTENT_SLEEP_INTERVAL = 3
    MIN_CHUNK_LENGTH = 64

    def __init__(self):
        # ä¸‰å±‚å‚æ•°è®¾è®¡ï¼šä¸»å‚æ•° + è‡ªåŠ¨è®¡ç®—
        self.content_fetch_size = int(os.getenv("PROCESSOR_CONTENT_FETCH_SIZE", "50"))
        self.chunk_buffer_limit = max(4, self.content_fetch_size // 2)
        self.chunk_batch_size = max(2, self.chunk_buffer_limit // 2)

        # æ ¸å¿ƒç»„ä»¶ï¼šåŒé‡chunker
        self.db_client = None
        self.db_operations = None
        self.current_chunker = SmartChunker()  # æ–°æ–¹æ¡ˆ
        self.deprecated_chunker = DeprecatedChunker()  # æ—§æ–¹æ¡ˆ

        # ç¼“å†²æ± 
        self.content_buffer: List[Tuple[str, str]] = []
        self.chunk_buffer: List[Dict[str, Any]] = []

        # ç»Ÿè®¡ä¿¡æ¯
        self.stats = {
            "total_processed": 0,
            "chunks_identical": 0,
            "chunks_different": 0,
            "embedding_saved": 0
        }

        logger.info(f"DualChunkingProcessor: content_fetch={self.content_fetch_size}, "
                   f"buffer_limit={self.chunk_buffer_limit}, batch={self.chunk_batch_size}")

    async def __aenter__(self):
        await self.initialize()
        return self

    async def __aexit__(self, _exc_type, _exc_val, _exc_tb):
        await self.cleanup()

    async def initialize(self) -> None:
        """Initialize database connections"""
        logger.info("Initializing dual chunking processor")
        self.db_client = create_database_client()
        await self.db_client.initialize()
        self.db_operations = DatabaseOperations(self.db_client)

    async def cleanup(self) -> None:
        """Clean up resources - å¤„ç†å‰©ä½™chunks"""
        # å¤„ç†å‰©ä½™çš„chunks
        if self.chunk_buffer:
            logger.info(f"Processing remaining {len(self.chunk_buffer)} chunks before cleanup")
            await self._execute_unified_batch()

        # è¾“å‡ºç»Ÿè®¡ä¿¡æ¯
        self._log_final_stats()

        if self.db_client:
            await self.db_client.close()
            logger.info("Database client closed")

    def _log_final_stats(self):
        """è¾“å‡ºæœ€ç»ˆç»Ÿè®¡ä¿¡æ¯"""
        total = self.stats["total_processed"]
        identical = self.stats["chunks_identical"]
        different = self.stats["chunks_different"]
        saved = self.stats["embedding_saved"]
        
        if total > 0:
            identical_pct = (identical / total) * 100
            saved_pct = (saved / total) * 100
            
            logger.info("=" * 60)
            logger.info("ğŸ“Š Dual Chunking Processor æœ€ç»ˆç»Ÿè®¡")
            logger.info("=" * 60)
            logger.info(f"æ€»å¤„ç†é¡µé¢: {total}")
            logger.info(f"ç»“æœä¸€è‡´: {identical} ({identical_pct:.1f}%)")
            logger.info(f"ç»“æœä¸åŒ: {different} ({100-identical_pct:.1f}%)")
            logger.info(f"èŠ‚çœembedding: {saved} ({saved_pct:.1f}%)")
            logger.info("=" * 60)

    async def start_processing(self) -> None:
        """å¯åŠ¨å¹¶å‘å¤„ç†å™¨æ± """
        logger.info("Starting dual chunking processor pool")
        await self._run_processor_pool()

    async def _run_processor_pool(self) -> None:
        """å¤„ç†å™¨æ± æ¶æ„ - ä¸‰ä¸ªç‹¬ç«‹å¹¶å‘è¿›ç¨‹"""
        try:
            # å¯åŠ¨ä¸‰ä¸ªç‹¬ç«‹è¿›ç¨‹
            content_supplier = asyncio.create_task(self._content_supplier())
            chunk_processor = asyncio.create_task(self._dual_chunk_processor())
            batch_manager = asyncio.create_task(self._batch_manager())

            logger.info("Dual chunking processor pool started: 3 concurrent processes")

            # æ‰€æœ‰è¿›ç¨‹å¹¶å‘è¿è¡Œ
            await asyncio.gather(content_supplier, chunk_processor, batch_manager)

        except KeyboardInterrupt:
            logger.info("Dual chunking processor pool interrupted by user")
        except Exception as e:
            logger.error(f"Dual chunking processor pool error: {e}")
            raise

    async def _content_supplier(self) -> None:
        """å†…å®¹ä¾›åº”å™¨ - ç‹¬ç«‹è¿›ç¨‹ï¼Œ50%é˜ˆå€¼è§¦å‘è¡¥å……"""
        while True:
            try:
                # 50%é˜ˆå€¼ç­–ç•¥ï¼šä½äº50%æ‰è¯·æ±‚ä¸‹ä¸€æ‰¹
                if len(self.content_buffer) < self.content_fetch_size // 2:
                    batch_results = await self.db_operations.get_process_urls_batch(self.content_fetch_size)
                    if batch_results:
                        self.content_buffer.extend(batch_results)
                        logger.debug(f"Content supplier: added {len(batch_results)} items")

                await asyncio.sleep(self.BUFFER_CHECK_INTERVAL)

            except Exception as e:
                logger.error(f"Content supplier error: {e}")
                await asyncio.sleep(self.NO_CONTENT_SLEEP_INTERVAL)

    def _compare_chunking_results(self, old_chunks: List[str], new_chunks: List[str]) -> bool:
        """å¯¹æ¯”ä¸¤ä¸ªchunkingç»“æœæ˜¯å¦ä¸€è‡´"""
        # 1. æ•°é‡å¯¹æ¯”
        if len(old_chunks) != len(new_chunks):
            return False
        
        # 2. å†…å®¹å¯¹æ¯”ï¼šé€ä¸ªæ¯”è¾ƒchunkå†…å®¹
        for old_chunk, new_chunk in zip(old_chunks, new_chunks):
            if old_chunk != new_chunk:
                return False
        
        return True

    async def _dual_chunk_processor(self) -> None:
        """åŒé‡å—å¤„ç†å™¨ - å¯¹æ¯”chunkingç»“æœï¼Œåªå¤„ç†ä¸ä¸€è‡´çš„å†…å®¹"""
        while True:
            try:
                # æµé‡æ§åˆ¶ï¼šè¶…è¿‡bufferé™åˆ¶æ—¶ç­‰å¾…
                if len(self.chunk_buffer) > self.chunk_buffer_limit:
                    await asyncio.sleep(0.1)
                    continue

                if self.content_buffer:
                    url, content = self.content_buffer.pop(0)

                    if content.strip():
                        # åŒé‡chunking
                        old_chunks = self.deprecated_chunker.chunk_text(content)
                        new_chunks = self.current_chunker.chunk_text(content)
                        
                        # æ™ºèƒ½å¯¹æ¯”
                        is_identical = self._compare_chunking_results(old_chunks, new_chunks)
                        
                        # æ›´æ–°ç»Ÿè®¡
                        self.stats["total_processed"] += 1
                        
                        if is_identical:
                            # ç»“æœä¸€è‡´ï¼Œè·³è¿‡å¤„ç†
                            self.stats["chunks_identical"] += 1
                            self.stats["embedding_saved"] += len(new_chunks)
                            logger.debug(f"Chunking identical for {url}, skipping embedding")
                        else:
                            # ç»“æœä¸ä¸€è‡´ï¼Œéœ€è¦å¤„ç†
                            self.stats["chunks_different"] += 1
                            
                            valid_chunks = [
                                chunk for chunk in new_chunks
                                if chunk.strip() and len(chunk) >= self.MIN_CHUNK_LENGTH
                            ]

                            for chunk in valid_chunks:
                                self.chunk_buffer.append({"url": url, "content": chunk})

                            if valid_chunks:
                                logger.debug(f"Chunking different for {url}, processing {len(valid_chunks)} chunks")

                    # æœ‰å†…å®¹æ—¶ç»§ç»­å¤„ç†ï¼Œä¸sleep
                    continue
                else:
                    # æ— å†…å®¹æ—¶æ‰sleep
                    await asyncio.sleep(0.1)

            except Exception as e:
                logger.error(f"Dual chunk processor error: {e}")
                await asyncio.sleep(self.NO_CONTENT_SLEEP_INTERVAL)

    async def _batch_manager(self) -> None:
        """æ‰¹å¤„ç†ç®¡ç†å™¨ - ç‹¬ç«‹è¿›ç¨‹ï¼Œå›ºå®š1ç§’é—´éš”æ£€æµ‹"""
        while True:
            try:
                if len(self.chunk_buffer) >= self.chunk_batch_size:
                    await self._execute_unified_batch()

                # å›ºå®š1ç§’é—´éš”æ£€æµ‹
                await asyncio.sleep(1.0)

            except Exception as e:
                logger.error(f"Batch manager error: {e}")
                await asyncio.sleep(self.NO_CONTENT_SLEEP_INTERVAL)

    async def _execute_unified_batch(self) -> None:
        """æ‰§è¡Œç»Ÿä¸€æ‰¹å¤„ç†ï¼šåŠ¨æ€äºŒåˆ†æ³•å¤„ç†APIé™åˆ¶"""
        if not self.chunk_buffer:
            return

        start_time = time.perf_counter()

        # åŠ¨æ€äºŒåˆ†æ³•å¤„ç†æ‰€æœ‰chunks
        all_embeddings = await self._adaptive_embedding_batch(self.chunk_buffer)

        # å‡†å¤‡å­˜å‚¨æ•°æ®ï¼ˆè·³è¿‡å¤±è´¥çš„chunksï¼‰
        valid_data = []
        for i, embedding in enumerate(all_embeddings):
            if embedding is not None:  # æˆåŠŸçš„embedding
                valid_data.append({
                    "url": self.chunk_buffer[i]["url"],
                    "content": self.chunk_buffer[i]["content"],
                    "embedding": str(embedding)
                })

        if valid_data:
            # æ‰¹é‡åˆ é™¤å’Œæ’å…¥
            urls_to_process = list(set(item["url"] for item in valid_data))
            await self.db_operations.delete_chunks_batch(urls_to_process)
            await self.db_operations.insert_chunks(valid_data)

            # æ ‡è®°é¡µé¢ä¸ºå·²å¤„ç†
            await self.db_operations.mark_pages_processed(urls_to_process)

        # ç»Ÿè®¡å’Œæ¸…ç†
        processing_time = time.perf_counter() - start_time
        skipped_count = len(self.chunk_buffer) - len(valid_data)
        logger.info(f"ğŸ“Š Dual batch completed: {len(valid_data)} processed, {skipped_count} skipped, {processing_time:.2f}s")

        self.chunk_buffer.clear()

    async def _adaptive_embedding_batch(self, chunk_items: List[Dict[str, Any]]) -> List[Any]:
        """åŠ¨æ€äºŒåˆ†æ³•æ‰¹é‡embedding - è‡ªé€‚åº”APIé™åˆ¶"""
        if not chunk_items:
            return []

        embedder = get_embedder()
        if not isinstance(embedder, SiliconFlowProvider):
            # æœ¬åœ°embeddingï¼Œé€ä¸ªå¤„ç†
            return [create_embedding(item["content"]) for item in chunk_items]

        # API embeddingï¼Œä½¿ç”¨åŠ¨æ€äºŒåˆ†æ³•
        return await self._binary_split_embedding(embedder, chunk_items)

    async def _binary_split_embedding(self, embedder, chunk_items: List[Dict[str, Any]], depth: int = 0) -> List[Any]:
        """é€’å½’äºŒåˆ†æ³•å¤„ç†APIé™åˆ¶"""
        if depth > 10:  # é˜²æ­¢æ— é™é€’å½’
            logger.error(f"Max recursion depth reached, skipping {len(chunk_items)} chunks")
            return [None] * len(chunk_items)

        chunk_texts = [item["content"] for item in chunk_items]

        try:
            # å°è¯•æ‰¹é‡å¤„ç†
            embeddings = await embedder.encode_batch_concurrent(chunk_texts)
            logger.info(f"âœ… Dual batch embedding: {len(chunk_texts)} chunks")
            return embeddings

        except Exception as e:
            if "413" in str(e) or "Request Entity Too Large" in str(e):
                # APIè¯·æ±‚è¿‡å¤§ï¼Œè¿›è¡ŒäºŒåˆ†
                if len(chunk_items) == 1:
                    # å•ä¸ªchunkéƒ½å¤ªå¤§ï¼Œè·³è¿‡
                    logger.warning(f"Single chunk too large, skipping: {len(chunk_texts[0])} chars")
                    return [None]

                # äºŒåˆ†å¤„ç†
                mid = len(chunk_items) // 2
                logger.info(f"API limit hit, splitting {len(chunk_items)} chunks into {mid} + {len(chunk_items) - mid}")

                left_embeddings = await self._binary_split_embedding(embedder, chunk_items[:mid], depth + 1)
                right_embeddings = await self._binary_split_embedding(embedder, chunk_items[mid:], depth + 1)

                return left_embeddings + right_embeddings
            else:
                # å…¶ä»–é”™è¯¯ï¼Œè·³è¿‡æ‰€æœ‰chunks
                logger.error(f"Embedding error: {e}, skipping {len(chunk_items)} chunks")
                return [None] * len(chunk_items)


async def main():
    """Main function"""
    async with DualChunkingProcessor() as processor:
        await processor.start_processing()


if __name__ == "__main__":
    asyncio.run(main())
