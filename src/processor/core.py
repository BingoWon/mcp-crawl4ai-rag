"""
Processor - è·¨URLæ‰¹é‡Embeddingå¤„ç†å™¨

å®ç°è·¨URL chunksæ”¶é›†å’Œæ‰¹é‡å¤„ç†ã€‚

æ ¸å¿ƒæœºåˆ¶ï¼š
- è·¨URLæ”¶é›†chunksåˆ°chunk_buffer
- è¾¾åˆ°é˜ˆå€¼æ—¶æ‰¹é‡å¤„ç†
- APIæ¨¡å¼ï¼šçœŸæ­£çš„æ‰¹é‡embedding (å•æ¬¡APIè°ƒç”¨)
- æœ¬åœ°æ¨¡å¼ï¼šé€ä¸ªembeddingå¤„ç†
- æ‰¹é‡storage (åˆ é™¤+æ’å…¥)

ç¯å¢ƒå˜é‡ï¼š
- PROCESSOR_CONTENT_FETCH_SIZE: å†…å®¹è·å–æ‰¹æ¬¡ (é»˜è®¤50)
- PROCESSOR_CHUNK_BATCH_SIZE: chunksæ‰¹å¤„ç†é˜ˆå€¼ (é»˜è®¤50)

ä½¿ç”¨æ–¹å¼ï¼š
    async with Processor() as processor:
        await processor.start_processing()
"""

import sys
import os
import asyncio
import time
from pathlib import Path
from typing import List, Dict, Any, Tuple
sys.path.insert(0, str(Path(__file__).parent.parent))

from database import create_database_client, DatabaseOperations
from chunking import SmartChunker
from embedding import create_embedding, get_embedder
from embedding.providers import SiliconFlowProvider
from utils.logger import setup_logger

logger = setup_logger(__name__)


class Processor:
    """è·¨URLæ‰¹é‡Embeddingå¤„ç†å™¨"""

    # ç³»ç»Ÿå¸¸é‡
    BUFFER_CHECK_INTERVAL = 1.0
    NO_CONTENT_SLEEP_INTERVAL = 3
    MIN_CHUNK_LENGTH = 128

    def __init__(self):
        # ä¸‰å±‚å‚æ•°è®¾è®¡ï¼šä¸»å‚æ•° + è‡ªåŠ¨è®¡ç®—
        self.content_fetch_size = int(os.getenv("PROCESSOR_CONTENT_FETCH_SIZE", "50"))
        self.chunk_buffer_limit = max(4, self.content_fetch_size // 2)
        self.chunk_batch_size = max(2, self.chunk_buffer_limit // 2)

        # æ ¸å¿ƒç»„ä»¶
        self.db_client = None
        self.db_operations = None
        self.chunker = SmartChunker()

        # ç¼“å†²æ± 
        self.content_buffer: List[Tuple[str, str]] = []
        self.chunk_buffer: List[Dict[str, Any]] = []

        logger.info(f"Processor: content_fetch={self.content_fetch_size}, "
                   f"buffer_limit={self.chunk_buffer_limit}, batch={self.chunk_batch_size}")

    async def __aenter__(self):
        await self.initialize()
        return self

    async def __aexit__(self, _exc_type, _exc_val, _exc_tb):
        await self.cleanup()

    async def initialize(self) -> None:
        """Initialize database connections"""
        logger.info("Initializing processor")
        self.db_client = create_database_client()
        await self.db_client.initialize()
        self.db_operations = DatabaseOperations(self.db_client)

    async def cleanup(self) -> None:
        """Clean up resources - å¤„ç†å‰©ä½™chunks"""
        # å¤„ç†å‰©ä½™çš„chunks
        if self.chunk_buffer:
            logger.info(f"Processing remaining {len(self.chunk_buffer)} chunks before cleanup")
            await self._execute_unified_batch()

        if self.db_client:
            await self.db_client.close()
            logger.info("Database client closed")

    async def start_processing(self) -> None:
        """å¯åŠ¨å¹¶å‘å¤„ç†å™¨æ±  - å…¨å±€æœ€ä¼˜è§£"""
        logger.info("Starting processor pool")
        await self._run_processor_pool()

    async def _run_processor_pool(self) -> None:
        """å¤„ç†å™¨æ± æ¶æ„ - ä¸‰ä¸ªç‹¬ç«‹å¹¶å‘è¿›ç¨‹"""
        try:
            # å¯åŠ¨ä¸‰ä¸ªç‹¬ç«‹è¿›ç¨‹
            content_supplier = asyncio.create_task(self._content_supplier())
            chunk_processor = asyncio.create_task(self._chunk_processor())
            batch_manager = asyncio.create_task(self._batch_manager())

            logger.info("Processor pool started: 3 concurrent processes")

            # æ‰€æœ‰è¿›ç¨‹å¹¶å‘è¿è¡Œ
            await asyncio.gather(content_supplier, chunk_processor, batch_manager)

        except KeyboardInterrupt:
            logger.info("Processor pool interrupted by user")
        except Exception as e:
            logger.error(f"Processor pool error: {e}")
            raise

    async def _content_supplier(self) -> None:
        """å†…å®¹ä¾›åº”å™¨ - ç‹¬ç«‹è¿›ç¨‹ï¼Œ50%é˜ˆå€¼è§¦å‘è¡¥å……"""
        while True:
            try:
                # 50%é˜ˆå€¼ç­–ç•¥ï¼šä½äº50%æ‰è¯·æ±‚ä¸‹ä¸€æ‰¹
                if len(self.content_buffer) < self.content_fetch_size // 2:
                    batch_results = await self.db_operations.get_process_urls_batch(self.content_fetch_size)
                    if batch_results:
                        self.content_buffer.extend(batch_results)
                        logger.debug(f"Content supplier: added {len(batch_results)} items")

                await asyncio.sleep(self.BUFFER_CHECK_INTERVAL)

            except Exception as e:
                logger.error(f"Content supplier error: {e}")
                await asyncio.sleep(self.NO_CONTENT_SLEEP_INTERVAL)

    async def _chunk_processor(self) -> None:
        """å—å¤„ç†å™¨ - ç‹¬ç«‹è¿›ç¨‹ï¼Œæµé‡æ§åˆ¶ + è¿ç»­å¤„ç†"""
        while True:
            try:
                # æµé‡æ§åˆ¶ï¼šè¶…è¿‡bufferé™åˆ¶æ—¶ç­‰å¾…
                if len(self.chunk_buffer) > self.chunk_buffer_limit:
                    await asyncio.sleep(0.1)
                    continue

                if self.content_buffer:
                    url, content = self.content_buffer.pop(0)

                    if content.strip():
                        chunks = self.chunker.chunk_text(content)
                        valid_chunks = [
                            chunk for chunk in chunks
                            if chunk.strip() and len(chunk) >= self.MIN_CHUNK_LENGTH
                        ]

                        for chunk in valid_chunks:
                            self.chunk_buffer.append({"url": url, "content": chunk})

                        if valid_chunks:
                            logger.debug(f"Chunk processor: processed {len(valid_chunks)} chunks")

                    # æœ‰å†…å®¹æ—¶ç»§ç»­å¤„ç†ï¼Œä¸sleep
                    continue
                else:
                    # æ— å†…å®¹æ—¶æ‰sleep
                    await asyncio.sleep(0.1)

            except Exception as e:
                logger.error(f"Chunk processor error: {e}")
                await asyncio.sleep(self.NO_CONTENT_SLEEP_INTERVAL)

    async def _batch_manager(self) -> None:
        """æ‰¹å¤„ç†ç®¡ç†å™¨ - ç‹¬ç«‹è¿›ç¨‹ï¼Œå›ºå®š1ç§’é—´éš”æ£€æµ‹"""
        while True:
            try:
                if len(self.chunk_buffer) >= self.chunk_batch_size:
                    await self._execute_unified_batch()

                # å›ºå®š1ç§’é—´éš”æ£€æµ‹
                await asyncio.sleep(1.0)

            except Exception as e:
                logger.error(f"Batch manager error: {e}")
                await asyncio.sleep(self.NO_CONTENT_SLEEP_INTERVAL)

    async def _execute_unified_batch(self) -> None:
        """æ‰§è¡Œç»Ÿä¸€æ‰¹å¤„ç†ï¼šembedding + storage ä¸€ä½“åŒ–"""
        if not self.chunk_buffer:
            return

        start_time = time.perf_counter()

        # 1. æå–æ‰€æœ‰chunksæ–‡æœ¬
        chunk_texts = [item["content"] for item in self.chunk_buffer]

        # 2. æ‰¹é‡embeddingå¤„ç† - çœŸæ­£çš„è·¨URLæ‰¹å¤„ç†
        embedder = get_embedder()
        if isinstance(embedder, SiliconFlowProvider):
            embeddings = await embedder.encode_batch_concurrent(chunk_texts)
            logger.info(f"âœ… True batch embedding: {len(chunk_texts)} chunks in single API call")
        else:
            embeddings = [create_embedding(chunk) for chunk in chunk_texts]
            logger.info(f"âœ… Local embedding: {len(chunk_texts)} chunks processed")

        # 3. å‡†å¤‡æ‰¹é‡å­˜å‚¨æ•°æ®
        all_data_to_insert = [
            {
                "url": self.chunk_buffer[i]["url"],
                "content": self.chunk_buffer[i]["content"],
                "embedding": str(embeddings[i])
            }
            for i in range(len(self.chunk_buffer))
        ]

        # 4. è·å–æ¶‰åŠçš„URLså¹¶æ‰¹é‡åˆ é™¤æ—§chunks
        urls_to_process = list(set(item["url"] for item in self.chunk_buffer))
        await self.db_operations.delete_chunks_batch(urls_to_process)

        # 5. æ‰¹é‡æ’å…¥æ–°chunks
        await self.db_operations.insert_chunks(all_data_to_insert)

        # 6. ç»Ÿè®¡å’Œæ¸…ç†
        processing_time = time.perf_counter() - start_time
        logger.info(f"ğŸ“Š Unified batch completed: {len(urls_to_process)} URLs, "
                   f"{len(all_data_to_insert)} chunks, {processing_time:.2f}s")

        # æ¸…ç©ºç¼“å†²æ± 
        self.chunk_buffer.clear()


async def main():
    """Main function"""
    async with Processor() as processor:
        await processor.start_processing()


if __name__ == "__main__":
    asyncio.run(main())
