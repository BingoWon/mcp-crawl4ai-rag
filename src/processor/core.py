"""
Pure Processor Core
çº¯å¤„ç†å™¨æ ¸å¿ƒæ¨¡å—

ä¸“æ³¨äºå†…å®¹å¤„ç†çš„ç‹¬ç«‹ç»„ä»¶ï¼Œæ˜¯ç»Ÿä¸€çˆ¬è™«ç³»ç»Ÿçš„å¤„ç†å¼•æ“ã€‚

=== ç»Ÿä¸€çˆ¬è™«ç³»ç»Ÿæ¶æ„ ===

æœ¬æ¨¡å—æ˜¯ç»Ÿä¸€çˆ¬è™«ç³»ç»Ÿçš„æ ¸å¿ƒç»„ä»¶ä¹‹ä¸€ï¼Œä¸çˆ¬å–å™¨ç»„ä»¶ååŒå·¥ä½œï¼š

**ç³»ç»Ÿæ¶æ„ï¼š**
- ç»Ÿä¸€å…¥å£ï¼štools/continuous_crawler.py å¹¶å‘è¿è¡Œçˆ¬å–å™¨å’Œå¤„ç†å™¨
- èŒè´£åˆ†ç¦»ï¼šçˆ¬å–å™¨ä¸“æ³¨çˆ¬å–ï¼Œå¤„ç†å™¨ä¸“æ³¨åˆ†å—åµŒå…¥
- æ•°æ®åº“åè°ƒï¼šé€šè¿‡ crawl_count å’Œ process_count å®ç°æ™ºèƒ½è°ƒåº¦

**å¤„ç†å™¨èŒè´£ï¼š**
- ä» pages è¡¨è¯»å–å·²çˆ¬å–çš„é¡µé¢å†…å®¹
- æ™ºèƒ½åˆ†å—å¤„ç†ï¼ˆH1/H2/H3åˆ†å±‚ç­–ç•¥ï¼‰
- å‘é‡åµŒå…¥ç”Ÿæˆï¼ˆQwen3-Embedding-4Bï¼‰
- chunks æ•°æ®å­˜å‚¨å’Œ process_count ç®¡ç†

=== å¤„ç†æµç¨‹è®¾è®¡ ===

**ä¼˜å…ˆçº§è°ƒåº¦ï¼š**
- åŸºäº process_count æœ€å°å€¼ä¼˜å…ˆå¤„ç†
- ç¡®ä¿æ‰€æœ‰é¡µé¢å¾—åˆ°å‡è¡¡å¤„ç†
- è‡ªåŠ¨å¹³è¡¡ç³»ç»Ÿè´Ÿè½½

**å¤„ç†æµç¨‹ï¼š**
1. è·å–æœ€å° process_count çš„é¡µé¢å†…å®¹
2. åˆ é™¤è¯¥URLçš„æ‰€æœ‰æ—§chunksï¼ˆç¡®ä¿æ•°æ®ä¸€è‡´æ€§ï¼‰
3. æ™ºèƒ½åˆ†å—ï¼šä½¿ç”¨SmartChunkerçš„åˆ†å±‚ç­–ç•¥
4. å‘é‡åµŒå…¥ï¼šä¸ºæ¯ä¸ªchunkç”Ÿæˆ2560ç»´åµŒå…¥å‘é‡
5. æ•°æ®å­˜å‚¨ï¼šæ’å…¥æ–°chunkså¹¶æ›´æ–°process_count

**å®¹é”™æœºåˆ¶ï¼š**
- ç©ºå†…å®¹é¡µé¢è·³è¿‡å¤„ç†ä½†æ›´æ–°è®¡æ•°
- åˆ†å—å¤±è´¥æ—¶è®°å½•é”™è¯¯ä½†ç»§ç»­å¤„ç†
- åµŒå…¥å¤±è´¥æ—¶è·³è¿‡è¯¥chunkä½†ä¸ä¸­æ–­æµç¨‹
"""

import sys
from pathlib import Path
sys.path.insert(0, str(Path(__file__).parent.parent))

from database import get_database_client, DatabaseOperations
from chunking import SmartChunker
from embedding import create_embedding, get_embedder
from embedding.providers import SiliconFlowProvider
from utils.logger import setup_logger
import asyncio
import os

logger = setup_logger(__name__)


class ContentProcessor:
    """å†…å®¹å¤„ç†å™¨ï¼Œä¸“æ³¨äºåˆ†å—å’ŒåµŒå…¥å¤„ç†"""

    def __init__(self):
        self.db_client = None
        self.db_operations = None
        self.chunker = SmartChunker()

    async def __aenter__(self):
        await self.initialize()
        return self

    async def __aexit__(self, exc_type, exc_val, exc_tb):
        await self.cleanup()

    async def initialize(self) -> None:
        """Initialize database connections"""
        logger.info("Initializing pure processor")
        self.db_client = await get_database_client()
        self.db_operations = DatabaseOperations(self.db_client)

    async def cleanup(self) -> None:
        """Clean up resources"""
        logger.info("Cleaning up processor resources")

    async def start_processing(self) -> None:
        """æ‰¹é‡å†…å®¹å¤„ç†å¾ªç¯ - å…¨å±€æœ€ä¼˜è§£"""
        batch_size = int(os.getenv("PROCESSOR_BATCH_SIZE", "5"))
        logger.info(f"ğŸš€ Starting batch processor (batch_size={batch_size})")

        process_count = 0
        while True:
            try:
                # æ‰¹é‡è·å–å¾…å¤„ç†çš„URLå’Œå†…å®¹
                batch_results = await self.db_operations.get_process_urls_batch(batch_size)

                if not batch_results:
                    logger.info("No URLs to process")
                    await asyncio.sleep(3)
                    continue

                logger.info(f"=== Processing batch of {len(batch_results)} URLs ===")

                # æ‰¹é‡å¤„ç†æ‰€æœ‰URLï¼ˆç§Ÿçº¦å·²åœ¨è·å–æ—¶å»ºç«‹ï¼‰
                processed_count = 0
                for url, content in batch_results:
                    process_count += 1
                    logger.info(f"Process #{process_count}: {url}")

                    try:
                        await self._process_content(url, content)
                        processed_count += 1
                    except Exception as e:
                        logger.error(f"Failed to process {url}: {e}")
                        continue

                logger.info(f"âœ… Batch completed: {processed_count}/{len(batch_results)} URLs processed")

            except KeyboardInterrupt:
                logger.info("Processor interrupted by user")
                break
            except Exception as e:
                logger.error(f"Batch process error: {e}")
                continue

    async def _process_content(self, url: str, content: str) -> None:
        """å¤„ç†é¡µé¢å†…å®¹ï¼šåˆ†å— + åµŒå…¥ + å­˜å‚¨ - å…¨å±€æœ€ä¼˜è§£"""
        logger.info(f"Processing content for: {url}")

        # Skip if no content
        if not content.strip():
            logger.error(f"âŒ No content to process for {url}")
            await self.db_operations.update_process_count(url)
            return

        # Delete old chunks and process content
        await self.db_operations.delete_chunks_by_url(url)
        chunks = self.chunker.chunk_text(content)

        if not chunks:
            logger.error(f"âŒ No chunks generated for {url}")
            await self.db_operations.update_process_count(url)
            return

        # Process chunks with embedding - æ™ºèƒ½ç­–ç•¥é€‰æ‹©
        data_to_insert = []
        valid_chunks = [chunk for chunk in chunks if chunk.strip()]

        if not valid_chunks:
            logger.error(f"âŒ No valid chunks for {url}")
            await self.db_operations.update_process_count(url)
            return

        # æ£€æµ‹embedding providerç±»å‹å¹¶é€‰æ‹©å¤„ç†ç­–ç•¥
        embedder = get_embedder()

        if isinstance(embedder, SiliconFlowProvider):
            # APIæ¨¡å¼ï¼šæ‰¹é‡å¹¶å‘å¤„ç†
            logger.info(f"API mode: batch processing {len(valid_chunks)} chunks")
            embeddings = await embedder.encode_batch_concurrent(valid_chunks)

            for i, (chunk, embedding) in enumerate(zip(valid_chunks, embeddings)):
                if len(chunk) < 128:
                    logger.error(f"âš ï¸ Chunk {i+1} é•¿åº¦è¿‡çŸ­: {len(chunk)} å­—ç¬¦")
                data_to_insert.append({
                    "url": url,
                    "content": chunk,
                    "embedding": str(embedding)
                })
        else:
            # æœ¬åœ°æ¨¡å¼ï¼šä¸¥æ ¼å•ä¸ªå¤„ç†
            logger.info(f"Local mode: sequential processing {len(valid_chunks)} chunks")
            for i, chunk in enumerate(valid_chunks):
                if len(chunk) < 128:
                    logger.error(f"âš ï¸ Chunk {i+1} é•¿åº¦è¿‡çŸ­: {len(chunk)} å­—ç¬¦")

                logger.info(f"Processing chunk {i+1}/{len(valid_chunks)}, length: {len(chunk)}")
                embedding = create_embedding(chunk)
                data_to_insert.append({
                    "url": url,
                    "content": chunk,
                    "embedding": str(embedding)
                })

        if not data_to_insert:
            logger.error(f"âŒ No data to insert for {url}")
            return

        # Insert chunks (process_count will be updated in batch)
        await self.db_operations.insert_chunks(data_to_insert)
        logger.info(f"âœ… Processed {url}: {len(data_to_insert)} chunks created")


async def main():
    """Main function for direct execution"""
    logger.info("ğŸš€ Pure Processor Starting")

    try:
        async with ContentProcessor() as processor:
            await processor.start_processing()
    except KeyboardInterrupt:
        logger.info("Processor interrupted by user")
    except Exception as e:
        logger.error(f"Processor error: {e}")


if __name__ == "__main__":
    try:
        asyncio.run(main())
    except KeyboardInterrupt:
        logger.info("Processor interrupted by user")
    except Exception as e:
        logger.error(f"Fatal error: {e}")
        import sys
        sys.exit(1)
