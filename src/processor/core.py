"""
æµæ°´çº¿Processorç³»ç»Ÿ - ä¼˜é›…ç°ä»£ç²¾ç®€çš„å…¨å±€æœ€ä¼˜è§£

æœ¬æ¨¡å—å®ç°äº†é’ˆå¯¹Local Embeddingç‰¹æ€§ä¼˜åŒ–çš„æµæ°´çº¿å¤„ç†æ¶æ„ï¼Œå®Œç¾è§£å†³ä¾›éœ€åŒ¹é…é—®é¢˜ã€‚
ç³»ç»Ÿé‡‡ç”¨å¤§é‡å†…å®¹è·å– + çº¿æ€§å¤„ç† + ç‹¬ç«‹å­˜å‚¨é˜ˆå€¼çš„æµæ°´çº¿è®¾è®¡ã€‚

ğŸ—ï¸ æ ¸å¿ƒæ¶æ„ï¼š
- å†…å®¹è·å–æ± ï¼šå¤§é‡è·å–å¾…å¤„ç†å†…å®¹ï¼Œç¡®ä¿ä¾›åº”å……è¶³
- çº¿æ€§å¤„ç†ï¼šé€‚é…Local Embeddingçš„çº¿æ€§ç‰¹æ€§ï¼Œæ— å¹¶å‘å†²çª
- ç»“æœç¼“å†²æ± ï¼šç‹¬ç«‹å­˜å‚¨é˜ˆå€¼ï¼Œæ‰¹é‡å­˜å‚¨ä¼˜åŒ–æ•°æ®åº“æ•ˆç‡
- æµæ°´çº¿è®¾è®¡ï¼šè·å–ã€å¤„ç†ã€å­˜å‚¨ä¸‰ä¸ªç¯èŠ‚ç‹¬ç«‹ä¼˜åŒ–

ğŸš€ æŠ€æœ¯ç‰¹æ€§ï¼š
- ä¾›éœ€å¹³è¡¡ï¼šè§£å†³Embeddingå¿«é€Ÿå¤„ç†(<1ç§’)çš„ä¾›éœ€åŒ¹é…é—®é¢˜
- çº¿æ€§ä¼˜åŒ–ï¼šå®Œç¾é€‚é…Localæ¨¡å‹å¿…é¡»çº¿æ€§å¤„ç†çš„ç‰¹æ€§
- æ‰¹é‡ä¼˜åŒ–ï¼šå¤§é‡è·å–å‡å°‘æ•°æ®åº“I/Oï¼Œæ‰¹é‡å­˜å‚¨æå‡æ•ˆç‡
- ç‹¬ç«‹æ§åˆ¶ï¼šè·å–ã€å¤„ç†ã€å­˜å‚¨ä¸‰ä¸ªé˜ˆå€¼ç‹¬ç«‹å¯æ§

âš¡ æ€§èƒ½ç‰¹å¾ï¼š
- å¤„ç†é€Ÿåº¦ï¼šEmbedding <1ç§’ï¼Œç³»ç»Ÿç“¶é¢ˆåªåœ¨å¤„ç†é€Ÿåº¦
- èµ„æºåˆ©ç”¨ï¼šå†…å®¹ä¾›åº”å……è¶³ï¼Œæ¨¡å‹ä¸ä¼šç©ºé—²ç­‰å¾…
- æ•°æ®åº“æ•ˆç‡ï¼šæ‰¹é‡æ“ä½œå‡å°‘90%çš„æ•°æ®åº“äº¤äº’
- æ‰©å±•æ€§ï¼šé˜ˆå€¼å‚æ•°å¯çµæ´»è°ƒæ•´ï¼Œé€‚åº”ä¸åŒåœºæ™¯

ğŸ¯ ä½¿ç”¨æ–¹å¼ï¼š
    async with StreamlineProcessor() as processor:
        await processor.start_processing()

âš™ï¸ ç¯å¢ƒå˜é‡é…ç½®ï¼š
- CONTENT_FETCH_SIZE: å†…å®¹è·å–æ‰¹æ¬¡å¤§å° (é»˜è®¤: 50)
- STORAGE_THRESHOLD: å­˜å‚¨é˜ˆå€¼ (é»˜è®¤: 30)

ğŸ¨ ä»£ç è´¨é‡ï¼š
- ä¼˜é›…åº¦ï¼šâ­â­â­â­â­ å¸¸é‡å®šä¹‰æ¸…æ™°ï¼Œæµæ°´çº¿æ¶æ„ä¼˜ç¾
- ç°ä»£åŒ–ï¼šâ­â­â­â­â­ ä½¿ç”¨æœ€æ–°Pythonç‰¹æ€§å’Œæœ€ä½³å®è·µ
- ç²¾ç®€åº¦ï¼šâ­â­â­â­â­ æ¶ˆé™¤æ‰€æœ‰å†—ä½™ï¼Œä»£ç æç®€
- æœ‰æ•ˆæ€§ï¼šâ­â­â­â­â­ å®Œç¾é€‚é…Local Embeddingç‰¹æ€§
"""

import sys
import os
import asyncio
import time
from pathlib import Path
from typing import List, Dict, Any, Tuple
sys.path.insert(0, str(Path(__file__).parent.parent))

from database import create_database_client, DatabaseOperations
from chunking import SmartChunker
from embedding import create_embedding, get_embedder
from embedding.providers import SiliconFlowProvider
from utils.logger import setup_logger

logger = setup_logger(__name__)


class StreamlineProcessor:
    """æµæ°´çº¿å¤„ç†å™¨ - ä¼˜é›…ç°ä»£ç²¾ç®€"""

    # å¸¸é‡å®šä¹‰ - æ¶ˆé™¤é­”æ³•æ•°å­—ï¼Œè€ƒè™‘Chunkingæ”¾å¤§æ•ˆåº”
    CONTENT_FETCH_SIZE = 50
    STORAGE_THRESHOLD = 10  # è€ƒè™‘chunkingæ”¾å¤§æ•ˆåº”ï¼Œé¿å…é¢‘ç¹å­˜å‚¨
    BUFFER_CHECK_INTERVAL = 1.0  # 1ç§’æ£€æŸ¥ï¼Œé¿å…é¢‘ç¹æ•°æ®åº“è®¿é—®
    NO_CONTENT_SLEEP_INTERVAL = 3
    MIN_CHUNK_LENGTH = 128

    def __init__(self):
        # æµæ°´çº¿ç»„ä»¶
        self.db_client = None
        self.db_operations = None
        self.chunker = SmartChunker()

        # æµæ°´çº¿ç¼“å†²æ± 
        self.content_buffer: List[Tuple[str, str]] = []
        self.result_buffer: List[Dict[str, Any]] = []

        # é…ç½®å‚æ•°
        self.content_fetch_size = int(os.getenv("CONTENT_FETCH_SIZE", str(self.CONTENT_FETCH_SIZE)))
        self.storage_threshold = int(os.getenv("STORAGE_THRESHOLD", str(self.STORAGE_THRESHOLD)))

        logger.info(f"Streamline Processor: fetch_size={self.content_fetch_size}, "
                   f"storage_threshold={self.storage_threshold}")

    async def __aenter__(self):
        await self.initialize()
        return self

    async def __aexit__(self, _exc_type, _exc_val, _exc_tb):
        await self.cleanup()

    async def initialize(self) -> None:
        """Initialize database connections"""
        logger.info("Initializing streamline processor")
        self.db_client = create_database_client()
        await self.db_client.initialize()
        self.db_operations = DatabaseOperations(self.db_client)

    async def cleanup(self) -> None:
        """Clean up resources"""
        logger.info("Cleaning up processor resources")

    async def start_processing(self) -> None:
        """æµæ°´çº¿å¤„ç†å¾ªç¯ - å…¨å±€æœ€ä¼˜è§£"""
        logger.info("Starting streamline processor")

        while True:
            try:
                # 1. ç¡®ä¿å†…å®¹ä¾›åº”å……è¶³
                await self._ensure_content_supply()

                # 2. çº¿æ€§å¤„ç†å•ä¸ªå†…å®¹
                if self.content_buffer:
                    await self._process_single_content()

                # 3. æ£€æŸ¥å¹¶æ‰¹é‡å­˜å‚¨
                await self._check_and_store()

                # 4. çŸ­æš‚ç­‰å¾…ï¼Œé¿å…CPUå ç”¨è¿‡é«˜
                await asyncio.sleep(self.BUFFER_CHECK_INTERVAL)

            except KeyboardInterrupt:
                logger.info("Streamline processor interrupted by user")
                break
            except Exception as e:
                logger.error(f"Streamline processor error: {e}")
                await asyncio.sleep(self.NO_CONTENT_SLEEP_INTERVAL)

    async def _ensure_content_supply(self) -> None:
        """ç¡®ä¿å†…å®¹ä¾›åº”å……è¶³ - å¤§é‡è·å–ç­–ç•¥"""
        if len(self.content_buffer) < self.content_fetch_size // 2:
            # å†…å®¹ä¸è¶³ï¼Œå¤§é‡è·å–è¡¥å……
            batch_results = await self.db_operations.get_process_urls_batch(self.content_fetch_size)

            if batch_results:
                self.content_buffer.extend(batch_results)
                logger.info(f"Content Supply: Added {len(batch_results)} contents, "
                           f"buffer size: {len(self.content_buffer)}")

    async def _process_single_content(self) -> None:
        """çº¿æ€§å¤„ç†å•ä¸ªå†…å®¹ - é€‚é…Local Embeddingç‰¹æ€§"""
        if not self.content_buffer:
            return

        url, content = self.content_buffer.pop(0)

        if not content.strip():
            return

        start_time = time.perf_counter()

        # åˆ†å—å¤„ç†
        chunks = self.chunker.chunk_text(content)
        valid_chunks = [
            chunk for chunk in chunks
            if chunk.strip() and len(chunk) >= self.MIN_CHUNK_LENGTH
        ]

        if not valid_chunks:
            return

        # çº¿æ€§embeddingå¤„ç† - ç°ä»£åŒ–æ¡ä»¶è¡¨è¾¾å¼
        embedder = get_embedder()
        embeddings = (
            await embedder.encode_batch_concurrent(valid_chunks)
            if isinstance(embedder, SiliconFlowProvider)
            else [create_embedding(chunk) for chunk in valid_chunks]
        )

        # æ·»åŠ åˆ°ç»“æœç¼“å†²æ± 
        result = {
            "url": url,
            "chunks": valid_chunks,
            "embeddings": embeddings
        }
        self.result_buffer.append(result)

        processing_time = time.perf_counter() - start_time
        logger.debug(f"Processed {url}: {len(valid_chunks)} chunks in {processing_time:.2f}s")

    async def _check_and_store(self) -> None:
        """æ£€æŸ¥å¹¶æ‰¹é‡å­˜å‚¨ - ç‹¬ç«‹å­˜å‚¨é˜ˆå€¼"""
        if len(self.result_buffer) >= self.storage_threshold:
            await self._flush_result_buffer()

    async def _flush_result_buffer(self) -> None:
        """æ¸…ç©ºç»“æœç¼“å†²æ±  - æ‰¹é‡å­˜å‚¨ä¼˜åŒ–"""
        if not self.result_buffer:
            return

        # ç°ä»£åŒ–æ•°æ®å¤„ç† - ä½¿ç”¨åˆ—è¡¨æ¨å¯¼å¼
        urls_to_process = [result["url"] for result in self.result_buffer]
        all_data_to_insert = [
            {
                "url": result["url"],
                "content": chunk,
                "embedding": str(embedding)
            }
            for result in self.result_buffer
            for chunk, embedding in zip(result["chunks"], result["embeddings"])
        ]

        # æ‰¹é‡åˆ é™¤æ—§chunks
        await self.db_operations.delete_chunks_batch(urls_to_process)

        # æ‰¹é‡æ’å…¥æ–°chunks
        if all_data_to_insert:
            await self.db_operations.insert_chunks(all_data_to_insert)

        logger.info(f"ğŸ“Š Stored {len(urls_to_process)} URLs, {len(all_data_to_insert)} chunks")

        # æ¸…ç©ºç¼“å†²æ± 
        self.result_buffer.clear()


async def main():
    """Main function"""
    async with StreamlineProcessor() as processor:
        await processor.start_processing()


if __name__ == "__main__":
    asyncio.run(main())
