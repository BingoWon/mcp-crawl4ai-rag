"""
Worker Poolçˆ¬è™«ç³»ç»Ÿ - ä¼˜é›…ç°ä»£ç²¾ç®€çš„å…¨å±€æœ€ä¼˜è§£

æœ¬æ¨¡å—å®ç°äº†åŸºäºWorker Poolæ¶æ„çš„é«˜æ€§èƒ½ç½‘é¡µçˆ¬è™«ç³»ç»Ÿï¼Œä¸“é—¨é’ˆå¯¹Appleå¼€å‘è€…æ–‡æ¡£ä¼˜åŒ–ã€‚
ç³»ç»Ÿé‡‡ç”¨å›ºå®šWorkeræ•°é‡çš„å¹¶å‘æ¶æ„ï¼Œå®Œå…¨è§£å†³äº†èµ„æºæ³„æ¼å’Œå¹¶å‘å¤±æ§é—®é¢˜ã€‚

ğŸ—ï¸ æ ¸å¿ƒæ¶æ„ï¼š
- Worker Poolæ¨¡å¼ï¼šå›ºå®šæ•°é‡çš„ç‹¬ç«‹å·¥ä½œå•å…ƒï¼Œèµ„æºä½¿ç”¨å®Œå…¨å¯æ§
- URLä¾›éœ€å¹³è¡¡ï¼šæ™ºèƒ½URLé˜Ÿåˆ—ç®¡ç†ï¼Œç¡®ä¿Workerä¸ç©ºé—²
- æ‰¹é‡å­˜å‚¨ä¼˜åŒ–ï¼šè¾¾åˆ°æ‰¹æ¬¡å¤§å°è‡ªåŠ¨å­˜å‚¨ï¼Œæ•°æ®åº“æ•ˆç‡æœ€ä¼˜
- åŒé‡é”æœºåˆ¶ï¼šæ•°æ®åº“Advisory Lock + åº”ç”¨Storage Lockï¼Œåˆ†å¸ƒå¼å®‰å…¨

ğŸš€ æŠ€æœ¯ç‰¹æ€§ï¼š
- å¼‚æ­¥å¹¶å‘ï¼šåŸºäºasyncioçš„ç°ä»£å¼‚æ­¥æ¶æ„
- åˆ†å¸ƒå¼å®‰å…¨ï¼šPostgreSQL Advisory Lockç¡®ä¿å¤šå®ä¾‹å®‰å…¨
- å†…å­˜å¯æ§ï¼šå›ºå®šèµ„æºæ± ï¼Œå†…å­˜ä½¿ç”¨ç¨³å®šå¯é¢„æµ‹
- æ€§èƒ½ä¼˜åŒ–ï¼šæ‰¹é‡æ“ä½œã€è¿æ¥å¤ç”¨ã€é”ç²’åº¦ä¼˜åŒ–
- ä¼˜é›…ç°ä»£ï¼šä½¿ç”¨æœ€æ–°Pythonç‰¹æ€§å’Œæœ€ä½³å®è·µ

ğŸ”’ åŒé‡é”æœºåˆ¶ï¼š
- æ•°æ®åº“å±‚ï¼šAdvisory Lockä¿æŠ¤URLè·å–çš„åˆ†å¸ƒå¼åŸå­æ€§
- åº”ç”¨å±‚ï¼šStorage Lockä¿æŠ¤å†…å­˜ç¼“å†²åŒºçš„å¹¶å‘å®‰å…¨
- èŒè´£åˆ†ç¦»ï¼šä¸¤å±‚é”ä½œç”¨åŸŸå®Œå…¨åˆ†ç¦»ï¼Œæ— å†²çªé£é™©
- æ€§èƒ½ä¼˜åŒ–ï¼šé”ç²’åº¦æœ€å°åŒ–ï¼Œé¿å…ä¸å¿…è¦ç­‰å¾…

ğŸ“Š æ€§èƒ½ç‰¹å¾ï¼š
- èµ„æºåˆ©ç”¨ç‡ï¼š90-98%ï¼ˆæ— ç©ºé—²æ—¶é—´ï¼‰
- å¹¶å‘æ§åˆ¶ï¼šå›ºå®šWorkeræ•°é‡ï¼Œå®Œå…¨å¯æ§
- å†…å­˜ä½¿ç”¨ï¼šç¨³å®šå¯é¢„æµ‹ï¼Œé€‚åˆé•¿æœŸè¿è¡Œ
- æ‰©å±•æ€§ï¼šè°ƒæ•´WORKER_BATCH_SIZEå³å¯çº¿æ€§æ‰©å±•

ğŸ¯ ä½¿ç”¨æ–¹å¼ï¼š
    async with BatchCrawler() as crawler:
        await crawler.start_crawling("https://developer.apple.com/documentation/swiftui")

âš™ï¸ ç¯å¢ƒå˜é‡é…ç½®ï¼š
- WORKER_BATCH_SIZE: ç»Ÿä¸€æ§åˆ¶Workeræ•°é‡å’Œæ‰¹å¤„ç†å¤§å° (é»˜è®¤: 5)
- CRAWLER_DUAL_CRAWL_ENABLED: æ˜¯å¦å¯ç”¨åŒé‡çˆ¬å–æ¨¡å¼ (é»˜è®¤: false)

ğŸ¯ å…¨å±€æœ€ä¼˜è§£ï¼š
- Workeræ•°é‡ = æ‰¹å¤„ç†å¤§å° = é˜Ÿåˆ—å¤§å° = WORKER_BATCH_SIZE
- å®Œç¾çš„1:1:1å¯¹åº”å…³ç³»ï¼Œæ¶ˆé™¤èµ„æºæµªè´¹å’Œç­‰å¾…æ—¶é—´

ğŸ”§ æŠ€æœ¯å‚æ•°ï¼ˆç¡¬ç¼–ç ï¼‰ï¼š
- STORAGE_CHECK_INTERVAL: å­˜å‚¨æ£€æŸ¥é—´éš”ï¼Œ30ç§’
- NO_URLS_SLEEP_INTERVAL: æ— URLæ—¶ç¡çœ é—´éš”ï¼Œ5ç§’
- URL_CHECK_INTERVAL: URLæ£€æŸ¥é—´éš”ï¼Œ1ç§’

ğŸ¨ ä»£ç è´¨é‡ï¼š
- ä¼˜é›…åº¦ï¼šâ­â­â­â­â­ å¸¸é‡å®šä¹‰æ¸…æ™°ï¼Œæ–¹æ³•èŒè´£å•ä¸€
- ç°ä»£åŒ–ï¼šâ­â­â­â­â­ ä½¿ç”¨æœ€æ–°Pythonç‰¹æ€§å’Œæœ€ä½³å®è·µ
- ç²¾ç®€åº¦ï¼šâ­â­â­â­â­ æ¶ˆé™¤æ‰€æœ‰å†—ä½™ï¼Œä»£ç æç®€
- æœ‰æ•ˆæ€§ï¼šâ­â­â­â­â­ åŠŸèƒ½å®Œæ•´ï¼Œæ€§èƒ½ä¼˜ç§€ï¼Œç¨³å®šå¯é 
"""

from typing import List, Optional, Tuple, Any, Dict
import sys
import os
from pathlib import Path
sys.path.insert(0, str(Path(__file__).parent.parent))
from database import create_database_client, DatabaseOperations
from .apple_stealth_crawler import CrawlerPool
from utils.logger import setup_logger
import asyncio
import time
from urllib.parse import urlparse, urlunparse

logger = setup_logger(__name__)


class Crawler:
    """Worker Poolçˆ¬è™«ç³»ç»Ÿ"""

    # å¸¸é‡å®šä¹‰ - æ¶ˆé™¤é­”æ³•æ•°å­—
    APPLE_DOCS_URL_PREFIX = "https://developer.apple.com/documentation/"
    NOT_FOUND_MESSAGE = "The page you're looking for can't be found."

    # å…¨å±€æœ€ä¼˜è§£å‚æ•° - ç»Ÿä¸€æ§åˆ¶
    WORKER_BATCH_SIZE = 5         # é»˜è®¤å€¼ï¼Œç¯å¢ƒå˜é‡å¯è¦†ç›–

    # æŠ€æœ¯å‚æ•° - ç¡¬ç¼–ç å¸¸é‡
    NO_URLS_SLEEP_INTERVAL = 5
    URL_CHECK_INTERVAL = 1
    STORAGE_CHECK_INTERVAL = 30

    def __init__(self):
        """Initialize Worker Pool Crawler - å…¨å±€æœ€ä¼˜è§£"""
        # ç»Ÿä¸€å‚æ•°æ§åˆ¶æ•´ä¸ªç³»ç»Ÿ - 1:1:1å®Œç¾å¯¹åº”
        self.worker_batch_size = int(os.getenv("WORKER_BATCH_SIZE", str(self.WORKER_BATCH_SIZE)))
        self.dual_crawl_enabled = os.getenv("CRAWLER_DUAL_CRAWL_ENABLED", "false").lower() == "true"

        # ç³»ç»Ÿç»„ä»¶
        self.db_client = None
        self.db_operations = None
        self.crawler_pool = None

        # Worker Poolæ ¸å¿ƒç»„ä»¶
        self.url_queue = None
        self.storage_buffer = []
        self.storage_lock = asyncio.Lock()

        logger.info(f"Worker Pool Crawler: worker_batch_size={self.worker_batch_size}, dual_crawl={self.dual_crawl_enabled}")
        logger.info(f"Global Optimal: workers=batch=queue={self.worker_batch_size} (1:1:1 perfect match)")
        logger.info(f"Tech Params: storage_interval={self.STORAGE_CHECK_INTERVAL}s, "
                   f"no_urls_sleep={self.NO_URLS_SLEEP_INTERVAL}s, url_check={self.URL_CHECK_INTERVAL}s")
        
    async def __aenter__(self):
        """Async context manager entry"""
        await self.initialize()
        return self
        
    async def __aexit__(self, _exc_type, _exc_val, _exc_tb):
        """Async context manager exit"""
        await self.cleanup()

    async def initialize(self) -> None:
        """Initialize Worker Pool Crawler - ä¼˜é›…ç°ä»£ç²¾ç®€"""
        logger.info("Initializing Worker Pool Crawler")

        # åˆå§‹åŒ–æ•°æ®åº“
        self.db_client = create_database_client()
        await self.db_client.initialize()
        self.db_operations = DatabaseOperations(self.db_client)

        # åˆå§‹åŒ–çˆ¬è™«æ± 
        self.crawler_pool = CrawlerPool(pool_size=self.worker_batch_size)
        await self.crawler_pool.initialize()

        # åˆå§‹åŒ–URLé˜Ÿåˆ— - 1:1:1å®Œç¾å¯¹åº”
        self.url_queue = asyncio.Queue(maxsize=self.worker_batch_size)

        logger.info(f"Worker Pool initialized: {self.worker_batch_size} workers")

    async def cleanup(self) -> None:
        """ä¼˜é›…çš„èµ„æºæ¸…ç†"""
        logger.info("Cleaning up batch crawler resources")

        # æ¸…ç†çˆ¬è™«æ± 
        if self.crawler_pool:
            await self.crawler_pool.close()
            self.crawler_pool = None
            logger.info("Crawler pool closed")

        # æ¸…ç†æ•°æ®åº“è¿æ¥
        if self.db_client:
            await self.db_client.close()
            self.db_client = None
            logger.info("Database connection closed")

    def clean_and_normalize_urls_batch(self, urls: List[str]) -> List[str]:
        """æ‰¹é‡æ¸…æ´—å’Œæ ‡å‡†åŒ–URL - ä¼˜é›…ç°ä»£ç²¾ç®€"""
        def normalize_url(url: str) -> str:
            parsed = urlparse(url)
            return urlunparse(parsed._replace(
                scheme=parsed.scheme.lower(),
                netloc=parsed.netloc.lower(),
                path=parsed.path.rstrip('/').lower(),
                query='',
                fragment=''
            ))

        return [normalize_url(url) for url in urls]

    async def start_crawling(self, start_url: str) -> None:
        """å¯åŠ¨Worker Poolçˆ¬è™« - å…¨å±€æœ€ä¼˜è§£"""
        if not self.db_operations:
            raise RuntimeError("Crawler not initialized. Use async with statement.")

        if not start_url.startswith(self.APPLE_DOCS_URL_PREFIX):
            logger.error("Only Apple documentation URLs are supported")
            return

        # Insert start URL if not exists
        await self.db_operations.insert_url_if_not_exists(start_url)
        crawl_mode = "dual" if self.dual_crawl_enabled else "single"
        logger.info(f"Starting Worker Pool Crawler: {self.worker_batch_size} workers, {crawl_mode} mode")

        # å¯åŠ¨Worker Poolæ¶æ„
        await self._run_worker_pool()

    async def _run_worker_pool(self) -> None:
        """Worker Poolæ¶æ„ - å…¨å±€æœ€ä¼˜è§£"""
        try:
            # å¯åŠ¨URLä¾›åº”å™¨
            url_supplier = asyncio.create_task(self._url_supplier())

            # å¯åŠ¨å­˜å‚¨ç®¡ç†å™¨
            storage_manager = asyncio.create_task(self._storage_manager())

            # å¯åŠ¨å›ºå®šæ•°é‡çš„worker - ç°ä»£åŒ–è¯­æ³•
            workers = [
                asyncio.create_task(self._crawler_worker(i))
                for i in range(self.worker_batch_size)
            ]

            logger.info(f"Worker Pool started: {self.worker_batch_size} workers")

            # ç­‰å¾…æ‰€æœ‰ç»„ä»¶
            await asyncio.gather(url_supplier, storage_manager, *workers)

        except KeyboardInterrupt:
            logger.info("Worker Pool interrupted by user")
        except Exception as e:
            logger.error(f"Worker Pool error: {e}")
            raise

    async def _url_supplier(self) -> None:
        """URLä¾›åº”å™¨ - ç»´æŒURLé˜Ÿåˆ—å……è¶³"""
        while True:
            try:
                if self.url_queue.qsize() < self.worker_batch_size:
                    # URLä¸è¶³ï¼Œæ‰¹é‡è·å–è¡¥å……
                    batch_urls = await self.db_operations.get_urls_batch(
                        self.worker_batch_size
                    )

                    if batch_urls:
                        # ç²¾ç®€çš„URLæ·»åŠ é€»è¾‘
                        for url in batch_urls:
                            if self.url_queue.full():
                                break
                            await self.url_queue.put(url)

                        logger.info(f"URL Supplier: Added {len(batch_urls)} URLs")
                    else:
                        await asyncio.sleep(self.NO_URLS_SLEEP_INTERVAL)
                else:
                    await asyncio.sleep(self.URL_CHECK_INTERVAL)

            except Exception as e:
                logger.error(f"URL Supplier error: {e}")
                await asyncio.sleep(self.NO_URLS_SLEEP_INTERVAL)

    async def _crawler_worker(self, worker_id: int) -> None:
        """Crawler Worker - ç‹¬ç«‹å·¥ä½œï¼Œå¤„ç†å•ä¸ªURL"""
        logger.info(f"Worker #{worker_id}: Started")

        while True:
            try:
                # ä»é˜Ÿåˆ—è·å–URL
                url = await self.url_queue.get()

                start_time = time.perf_counter()
                logger.debug(f"Worker #{worker_id}: Processing {url}")

                # çˆ¬å–URL
                result = await self._crawl_single_url(url)

                # æ·»åŠ åˆ°å­˜å‚¨ç¼“å†²
                await self._add_to_storage_buffer(result)

                # æ€§èƒ½ç»Ÿè®¡ - ç°ä»£åŒ–æ—¶é—´æµ‹é‡
                processing_time = time.perf_counter() - start_time
                logger.debug(f"Worker #{worker_id}: Completed {url} in {processing_time:.2f}s")

                # æ ‡è®°ä»»åŠ¡å®Œæˆ
                self.url_queue.task_done()

            except asyncio.CancelledError:
                logger.info(f"Worker #{worker_id}: Cancelled")
                break
            except Exception as e:
                logger.error(f"Worker #{worker_id}: Error processing URL: {e}")
                self.url_queue.task_done()  # å³ä½¿å‡ºé”™ä¹Ÿè¦æ ‡è®°å®Œæˆ

    async def _crawl_single_url(self, url: str) -> Dict[str, Any]:
        """çˆ¬å–å•ä¸ªURL - 404æ£€æµ‹ä¼˜åŒ–"""
        try:
            # å†…å®¹çˆ¬å–ï¼ˆå§‹ç»ˆæ‰§è¡Œï¼‰
            content, links_data = await self.crawler_pool.crawl_page(url, "#app-main")

            discovered_links = []
            is_404 = False

            # é“¾æ¥çˆ¬å–ï¼ˆæ ¹æ®é…ç½®å†³å®šï¼‰
            if self.dual_crawl_enabled:
                # åŒé‡çˆ¬å–æ¨¡å¼ï¼šå®Œæ•´é¡µé¢çˆ¬å–ç”¨äºé“¾æ¥æå–å’Œ404æ£€æµ‹
                full_content, links_data = await self.crawler_pool.crawl_page(url)

                # 404æ£€æµ‹ï¼šæ£€æŸ¥å®Œæ•´é¡µé¢å†…å®¹
                if full_content and self.NOT_FOUND_MESSAGE in full_content:
                    is_404 = True

                if links_data:
                    discovered_links = self._extract_links_from_data(links_data)
            else:
                # å•æ¬¡çˆ¬å–æ¨¡å¼ï¼šä»å†…å®¹çˆ¬å–çš„é“¾æ¥æ•°æ®ä¸­æå–
                if links_data:
                    discovered_links = self._extract_links_from_data(links_data)

            return {
                "url": url,
                "content": content or "",
                "discovered_links": discovered_links,
                "is_404": is_404
            }

        except Exception as e:
            logger.error(f"Failed to crawl {url}: {e}")
            return {
                "url": url,
                "content": "",
                "discovered_links": [],
                "is_404": False
            }

    async def _add_to_storage_buffer(self, result: Dict[str, Any]) -> None:
        """æ·»åŠ ç»“æœåˆ°å­˜å‚¨ç¼“å†² - ä¼˜åŒ–é”ç²’åº¦"""
        should_flush = False

        async with self.storage_lock:
            self.storage_buffer.append(result)
            should_flush = len(self.storage_buffer) >= self.worker_batch_size

        # åœ¨é”å¤–æ‰§è¡Œè€—æ—¶æ“ä½œ
        if should_flush:
            await self._flush_storage_buffer()

    async def _storage_manager(self) -> None:
        """å­˜å‚¨ç®¡ç†å™¨ - å®šæœŸæ¸…ç©ºç¼“å†²ï¼Œé˜²æ­¢æ•°æ®å»¶è¿Ÿ"""
        while True:
            try:
                await asyncio.sleep(self.STORAGE_CHECK_INTERVAL)

                # æ£€æŸ¥æ˜¯å¦éœ€è¦æ¸…ç©ºç¼“å†²
                should_flush = False
                buffer_size = 0

                async with self.storage_lock:
                    if self.storage_buffer:
                        should_flush = True
                        buffer_size = len(self.storage_buffer)

                if should_flush:
                    logger.info(f"Storage Manager: Flushing {buffer_size} pending results")
                    await self._flush_storage_buffer()

            except Exception as e:
                logger.error(f"Storage Manager error: {e}")

    async def _flush_storage_buffer(self) -> None:
        """æ¸…ç©ºå­˜å‚¨ç¼“å†² - 404æ£€æµ‹ä¼˜åŒ–"""
        # è·å–ç¼“å†²æ•°æ®å¹¶æ¸…ç©º
        buffer_data = []
        async with self.storage_lock:
            if not self.storage_buffer:
                return
            buffer_data = self.storage_buffer.copy()
            self.storage_buffer.clear()

        # åˆ†ç¦»æœ‰æ•ˆæ•°æ®å’Œ404æ•°æ®
        url_content_pairs, all_discovered_links, invalid_urls = self._separate_buffer_data(buffer_data)

        # å­˜å‚¨æœ‰æ•ˆæ•°æ®
        if url_content_pairs:
            await self._store_pages_and_links(url_content_pairs, all_discovered_links)

        # åˆ é™¤404 URL
        if invalid_urls:
            deleted_count = await self.db_operations.delete_pages_batch(invalid_urls)
            logger.warning(f"ğŸ—‘ï¸ Deleted {deleted_count} invalid URLs (404 pages)")

    def _separate_buffer_data(self, buffer_data: List[Dict[str, Any]]) -> Tuple[List[Tuple[str, str]], List[str], List[str]]:
        """åˆ†ç¦»ç¼“å†²æ•°æ® - ä¼˜é›…ç°ä»£ç²¾ç®€"""
        # åˆ†ç¦»æœ‰æ•ˆç»“æœå’Œ404ç»“æœ
        valid_results = [r for r in buffer_data if not r.get("is_404", False)]
        invalid_urls = [r["url"] for r in buffer_data if r.get("is_404", False)]

        # æå–æœ‰æ•ˆæ•°æ®
        url_content_pairs = [(r["url"], r["content"]) for r in valid_results]
        all_discovered_links = [link for r in valid_results for link in r["discovered_links"]]

        return url_content_pairs, all_discovered_links, invalid_urls

    async def _store_pages_and_links(self, url_content_pairs: List[Tuple[str, str]],
                                   all_discovered_links: List[str]) -> None:
        """æ‰¹é‡å­˜å‚¨é¡µé¢å’Œé“¾æ¥ - ä¼˜é›…ç°ä»£ç²¾ç®€"""
        # æ‰¹é‡æ›´æ–°é¡µé¢å†…å®¹
        if url_content_pairs:
            valid_count, empty_count = await self.db_operations.update_pages_batch(url_content_pairs)
            logger.info(f"ğŸ“Š Stored {len(url_content_pairs)} pages: {valid_count} valid, {empty_count} empty")

        # å­˜å‚¨å‘ç°çš„é“¾æ¥
        if all_discovered_links:
            await self._store_discovered_links(all_discovered_links)
            logger.info(f"ğŸ”— Discovered {len(all_discovered_links)} new links")

    async def _store_discovered_links(self, links: List[str]) -> None:
        """æ‰¹é‡å­˜å‚¨å‘ç°çš„é“¾æ¥ - ä¼˜é›…ç°ä»£ç²¾ç®€"""
        if not links:
            return

        # æ‰¹é‡æ¸…ç†å’Œè¿‡æ»¤Appleæ–‡æ¡£é“¾æ¥
        cleaned_links = self.clean_and_normalize_urls_batch(links)
        apple_links = [link for link in cleaned_links if link.startswith(self.APPLE_DOCS_URL_PREFIX)]

        if apple_links:
            # æ‰¹é‡æ’å…¥æ•°æ®åº“
            new_count = await self.db_operations.insert_urls_batch(apple_links)
            if new_count > 0:
                logger.info(f"Added {new_count} new URLs to crawl queue")

    def _extract_links_from_data(self, links_data: Optional[Dict[str, Any]]) -> List[str]:
        """Extract all internal links from crawl results"""
        if not links_data or not links_data.get("internal"):
            return []

        extracted_links = []
        for link in links_data["internal"]:
            # crawl4aiå·²ç¡®ä¿linkæ˜¯dictä¸”åŒ…å«href
            extracted_links.append(link["href"])

        return list(set(extracted_links))  # Remove duplicates
