"""
BatchCrawler - ç®€åŒ–åŒé‡çˆ¬å–æ‰¹é‡çˆ¬è™«
æ‰¹é‡å¹¶å‘ç½‘é¡µçˆ¬è™«ï¼Œé‡‡ç”¨ç®€åŒ–çš„åŒé‡çˆ¬å–ç­–ç•¥

ä¸“æ³¨äºé«˜æ•ˆçš„æ‰¹é‡ç½‘é¡µçˆ¬å–å’Œå®Œæ•´é“¾æ¥å‘ç°çš„ç‹¬ç«‹ç»„ä»¶ï¼Œæ˜¯ç»Ÿä¸€çˆ¬è™«ç³»ç»Ÿçš„æ ¸å¿ƒçˆ¬å–å¼•æ“ã€‚

=== ç®€åŒ–åŒé‡çˆ¬å–æ¶æ„ ===

æœ¬æ¨¡å—å®ç°äº†ç®€åŒ–çš„åŒé‡çˆ¬å–ç­–ç•¥ï¼Œç¡®ä¿é€»è¾‘æ¸…æ™°å’Œå®Œæ•´è¦†ç›–ï¼š

**æ ¸å¿ƒç­–ç•¥ï¼š**
- ç»Ÿä¸€åŒé‡çˆ¬å–ï¼šæ¯ä¸ªURLéƒ½è¿›è¡Œä¸¤æ¬¡çˆ¬å–ï¼Œæ— ä¾‹å¤–
- ç¬¬ä¸€æ¬¡çˆ¬å–ï¼šå¸¦CSSé€‰æ‹©å™¨("#app-main")ï¼Œä¸“é—¨è·å–é¡µé¢æ ¸å¿ƒå†…å®¹
- ç¬¬äºŒæ¬¡çˆ¬å–ï¼šä¸å¸¦CSSé€‰æ‹©å™¨ï¼Œä¸“é—¨è·å–å®Œæ•´é¡µé¢é“¾æ¥
- å¹¶å‘æ‰§è¡Œï¼šæ‰€æœ‰çˆ¬å–ä»»åŠ¡å¹¶å‘å¤„ç†ï¼Œæœ€å¤§åŒ–æ€§èƒ½

**ç³»ç»Ÿæ¶æ„ï¼š**
- ç»Ÿä¸€å…¥å£ï¼štools/continuous_crawler.py è¿è¡Œæ‰¹é‡çˆ¬å–å™¨
- èŒè´£åˆ†ç¦»ï¼šçˆ¬å–å™¨ä¸“æ³¨å†…å®¹å’Œé“¾æ¥è·å–ï¼Œå¤„ç†å™¨ä¸“æ³¨åˆ†å—åµŒå…¥
- æ•°æ®åº“åè°ƒï¼šé€šè¿‡ crawl_count å®ç°æ™ºèƒ½è°ƒåº¦
- è¿æ¥æ± å¤ç”¨ï¼šå¤ç”¨æµè§ˆå™¨å®ä¾‹ï¼Œå‡å°‘å¯åŠ¨å¼€é”€

**çˆ¬å–å™¨èŒè´£ï¼š**
- æ‰¹é‡ç½‘é¡µå†…å®¹çˆ¬å–å’Œå­˜å‚¨åˆ° pages è¡¨
- å®Œæ•´é“¾æ¥å‘ç°å’Œæ–°URLå­˜å‚¨
- crawl_count è®¡æ•°ç®¡ç†

=== åŒé‡çˆ¬å–ç­–ç•¥è¯¦è§£ ===

**ç­–ç•¥è®¾è®¡åŸç†ï¼š**
- å†…å®¹è·å–ï¼šä½¿ç”¨CSSé€‰æ‹©å™¨è¿‡æ»¤ï¼Œè·å–é¡µé¢æ ¸å¿ƒå†…å®¹ç”¨äºå­˜å‚¨å’Œå¤„ç†
- é“¾æ¥å‘ç°ï¼šä¸ä½¿ç”¨CSSé€‰æ‹©å™¨ï¼Œè·å–å®Œæ•´é¡µé¢æ‰€æœ‰é“¾æ¥ç”¨äºURLæ± æ‰©å±•
- èŒè´£åˆ†ç¦»ï¼šå†…å®¹å’Œé“¾æ¥è·å–å®Œå…¨ç‹¬ç«‹ï¼Œé¿å…ç›¸äº’å¹²æ‰°
- è¦†ç›–å®Œæ•´ï¼šç¡®ä¿æ¯ä¸ªé¡µé¢çš„å†…å®¹å’Œé“¾æ¥éƒ½è¢«å®Œæ•´è·å–

**æ‰¹é‡å¤„ç†æµç¨‹ï¼š**
1. æ‰¹é‡è·å–ï¼šä¸€æ¬¡è·å–å¤šä¸ªæœ€å° crawl_count çš„é¡µé¢
2. ä»»åŠ¡åˆ›å»ºï¼šä¸ºæ¯ä¸ªURLåˆ›å»ºå†…å®¹çˆ¬å–å’Œé“¾æ¥çˆ¬å–ä¸¤ä¸ªä»»åŠ¡
3. å¹¶å‘æ‰§è¡Œï¼šä½¿ç”¨è¿æ¥æ± å¹¶å‘å¤„ç†æ‰€æœ‰ä»»åŠ¡
4. ç»“æœåˆ†ç¦»ï¼šåˆ†åˆ«å¤„ç†å†…å®¹ç»“æœå’Œé“¾æ¥ç»“æœ
5. æ‰¹é‡å­˜å‚¨ï¼šæ‰¹é‡æ›´æ–°æ•°æ®åº“ï¼Œå‡å°‘I/Oå¼€é”€

**æ€§èƒ½ç‰¹ç‚¹ï¼š**
- é€»è¾‘ç®€å•ï¼šæ— å¤æ‚æ¡ä»¶åˆ¤æ–­ï¼Œæ˜“äºç†è§£å’Œç»´æŠ¤
- è¦†ç›–å®Œæ•´ï¼šæ¯ä¸ªURLéƒ½è¿›è¡Œå®Œæ•´çš„å†…å®¹å’Œé“¾æ¥è·å–
- å¹¶å‘é«˜æ•ˆï¼šè¿æ¥æ± å¤ç”¨ + æ‰¹é‡å¹¶å‘å¤„ç†
- æ•°æ®åº“ä¼˜åŒ–ï¼šæ‰¹é‡æ“ä½œå‡å°‘æ•°æ®åº“äº¤äº’æ¬¡æ•°
"""

from typing import List
import sys
import os
from pathlib import Path
sys.path.insert(0, str(Path(__file__).parent.parent))
from database import get_database_client, DatabaseOperations
from .apple_stealth_crawler import CrawlerPool
from utils.logger import setup_logger
import asyncio
from urllib.parse import urlparse, urlunparse

logger = setup_logger(__name__)


class BatchCrawler:
    """æ‰¹é‡å¹¶å‘ç½‘é¡µçˆ¬è™«ï¼Œä¸“æ³¨äºé«˜æ•ˆçš„æ‰¹é‡çˆ¬å–å’Œé“¾æ¥å‘ç°"""

    # Appleæ–‡æ¡£URLå¸¸é‡
    APPLE_DOCS_URL_PREFIX = "https://developer.apple.com/documentation/"

    def __init__(self):
        """Initialize batch crawler with environment-based configuration"""
        self.batch_size = int(os.getenv("CRAWLER_BATCH_SIZE", "30"))
        self.max_concurrent = int(os.getenv("CRAWLER_MAX_CONCURRENT", "30"))
        self.db_client = None
        self.db_operations = None
        self.crawler_pool = None
        
    async def __aenter__(self):
        """Async context manager entry"""
        await self.initialize()
        return self
        
    async def __aexit__(self, _exc_type, _exc_val, _exc_tb):
        """Async context manager exit"""
        await self.cleanup()

    async def initialize(self) -> None:
        """Initialize database connections and crawler pool"""
        logger.info("Initializing batch crawler with persistent connection pool")

        # Initialize NEON client
        self.db_client = await get_database_client()
        self.db_operations = DatabaseOperations(self.db_client)

        # Initialize persistent crawler pool
        self.crawler_pool = CrawlerPool(pool_size=self.max_concurrent)
        await self.crawler_pool.initialize()
        logger.info(f"Persistent crawler pool initialized with {self.max_concurrent} instances")

    async def cleanup(self) -> None:
        """Clean up resources including persistent crawler pool"""
        logger.info("Cleaning up batch crawler resources")

        # Close persistent crawler pool
        if self.crawler_pool:
            await self.crawler_pool.close()
            self.crawler_pool = None
            logger.info("Persistent crawler pool closed")

    def clean_and_normalize_urls_batch(self, urls: List[str]) -> List[str]:
        """æ‰¹é‡æ¸…æ´—å’Œæ ‡å‡†åŒ–URL - å…¨å±€æœ€ä¼˜è§£"""
        cleaned_urls = []
        for url in urls:
            parsed = urlparse(url)
            # æ¸…æ´—ï¼šç§»é™¤fragmentï¼Œæ ‡å‡†åŒ–ï¼šç§»é™¤æœ«å°¾æ–œæ 
            cleaned_parsed = parsed._replace(fragment='', path=parsed.path.rstrip('/'))
            cleaned_urls.append(urlunparse(cleaned_parsed))
        return cleaned_urls


    async def start_crawling(self, start_url: str) -> None:
        """å¼€å§‹æ‰¹é‡çˆ¬å–å¾ªç¯"""
        if not self.db_operations:
            raise RuntimeError("Crawler not initialized. Use async with statement.")

        if not start_url.startswith(self.APPLE_DOCS_URL_PREFIX):
            logger.error("Only Apple documentation URLs are supported")
            return

        # Insert start URL if not exists
        await self.db_operations.insert_url_if_not_exists(start_url)
        logger.info(f"Starting batch crawler from: {start_url} (batch_size={self.batch_size}, max_concurrent={self.max_concurrent})")

        batch_count = 0
        while True:
            try:
                # Get batch of URLs to crawl (minimum crawl_count)
                batch_urls = await self.db_operations.get_urls_batch(self.batch_size)
                if not batch_urls:
                    logger.info("No URLs to crawl")
                    break

                batch_count += 1
                logger.info(f"=== Batch #{batch_count}: Processing {len(batch_urls)} URLs ===")

                # Process batch concurrently
                await self._process_batch(batch_urls)

            except KeyboardInterrupt:
                logger.info("Batch crawl interrupted by user")
                break
            except Exception as e:
                logger.error(f"Batch crawl error: {e}")
                continue

    async def _process_batch(self, batch_urls: List[str]) -> None:
        """ä¼˜åŒ–çš„åŒé‡çˆ¬å–æ‰¹é‡å¤„ç† - ä½¿ç”¨æŒä¹…è¿æ¥æ± """
        logger.info(f"Batch processing: {len(batch_urls)} URLs with persistent crawler pool")

        if not self.crawler_pool:
            raise RuntimeError("Crawler pool not initialized. Use async with statement.")

        all_tasks = []

        # ä¸ºæ¯ä¸ªURLåˆ›å»ºä¸¤ä¸ªä»»åŠ¡ï¼šå†…å®¹çˆ¬å– + é“¾æ¥çˆ¬å–
        for url in batch_urls:
            # ç¬¬ä¸€æ¬¡çˆ¬å–ï¼šå¸¦CSSé€‰æ‹©å™¨ï¼Œè·å–å†…å®¹
            content_task = self.crawler_pool.crawl_page(url, "#app-main")
            all_tasks.append((url, content_task, "content"))

            # ç¬¬äºŒæ¬¡çˆ¬å–ï¼šä¸å¸¦CSSé€‰æ‹©å™¨ï¼Œè·å–é“¾æ¥
            # links_task = self.crawler_pool.crawl_page(url)
            # all_tasks.append((url, links_task, "links"))
            # TODO: åªçˆ¬å–ä¸€æ¬¡åˆ™é€šè¿‡ä¸‹é¢çš„é…ç½®ï¼š
            all_tasks.append((url, content_task, "links"))

        # å¹¶å‘æ‰§è¡Œæ‰€æœ‰ä»»åŠ¡
        results = await asyncio.gather(*[task for _, task, _ in all_tasks], return_exceptions=True)

        # å¤„ç†ç»“æœ
        await self._save_dual_results(batch_urls, results, all_tasks)

    async def _save_dual_results(self, batch_urls: List[str],
                               crawl_results: List, all_tasks: List) -> None:
        """ä¿å­˜åŒé‡çˆ¬å–ç»“æœåˆ°æ•°æ®åº“"""
        url_content_pairs = []
        all_discovered_links = []

        # æŒ‰ä»»åŠ¡ç±»å‹åˆ†ç»„å¤„ç†ç»“æœ
        content_results = {}

        for i, (url, _, task_type) in enumerate(all_tasks):
            result = crawl_results[i]
            if isinstance(result, Exception):
                logger.error(f"âŒ Failed to crawl {url} ({task_type}): {result}")
                continue

            if task_type == "content":
                content, _ = result
                content_results[url] = content
            elif task_type == "links":
                _, links_data = result
                if links_data:
                    extracted_links = self._extract_links_from_data(links_data)
                    all_discovered_links.extend(extracted_links)

        # å‡†å¤‡å†…å®¹æ›´æ–°æ•°æ®
        for url in batch_urls:
            content = content_results.get(url, "")
            url_content_pairs.append((url, content))

        # æ‰¹é‡é€‰æ‹©æ€§æ›´æ–°æ•°æ®åº“ - å…¨å±€æœ€ä¼˜è§£
        if url_content_pairs:
            valid_count, empty_count = await self.db_operations.update_pages_batch(url_content_pairs)
            logger.info(f"ğŸ“Š Content update stats: {valid_count} valid, {empty_count} empty")

        # å­˜å‚¨å‘ç°çš„é“¾æ¥
        if all_discovered_links:
            await self._store_discovered_links(all_discovered_links)
            logger.info(f"âœ… Batch processed: {len(url_content_pairs)} pages ({valid_count} valid content), {len(all_discovered_links)} new links discovered")
        else:
            logger.info(f"âœ… Batch processed: {len(url_content_pairs)} pages ({valid_count} valid content), no new links discovered")

    async def _store_discovered_links(self, links: list[str]) -> None:
        """æ‰¹é‡å­˜å‚¨å‘ç°çš„é“¾æ¥ - å…¨å±€æœ€ä¼˜è§£"""
        if not links:
            return

        # æ‰¹é‡æ¸…ç†å’Œè¿‡æ»¤Appleæ–‡æ¡£é“¾æ¥
        cleaned_links = self.clean_and_normalize_urls_batch(links)
        apple_links = [link for link in cleaned_links if link.startswith(self.APPLE_DOCS_URL_PREFIX)]

        if not apple_links:
            return

        # æ‰¹é‡æ’å…¥æ•°æ®åº“
        new_count = await self.db_operations.insert_urls_batch(apple_links)

        if new_count > 0:
            logger.info(f"Added {new_count} new URLs to crawl queue")



    def _extract_links_from_data(self, links_data) -> list[str]:
        """Extract all internal links from crawl results"""
        if not links_data or not links_data.get("internal"):
            return []

        extracted_links = []
        for link in links_data["internal"]:
            # crawl4aiå·²ç¡®ä¿linkæ˜¯dictä¸”åŒ…å«href
            extracted_links.append(link["href"])

        return list(set(extracted_links))  # Remove duplicates


