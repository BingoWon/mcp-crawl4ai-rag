#!/usr/bin/env python3
"""
æµ‹è¯•ä¿®å¤åçš„çˆ¬è™«é›†æˆåŠŸèƒ½
"""

import asyncio
import sys
from pathlib import Path

# æ·»åŠ srcç›®å½•åˆ°è·¯å¾„
sys.path.insert(0, str(Path(__file__).parent.parent / "src"))

from crawler.core import IndependentCrawler
from database.client import PostgreSQLClient


async def test_single_page_chunking():
    """æµ‹è¯•å•é¡µé¢åˆ†å—å­˜å‚¨"""
    print("ğŸ§ª æµ‹è¯•å•é¡µé¢åˆ†å—å­˜å‚¨...")
    
    # æ¨¡æ‹Ÿmarkdownå†…å®¹
    test_markdown = """# Apple Documentation Test
This is the main title content.

## Overview
This is the overview section with important information.

## First Section
This is the first section with detailed content about the topic.

## Second Section
This is the second section with more specific information."""

    async with IndependentCrawler() as crawler:
        # ç›´æ¥è°ƒç”¨å†…éƒ¨æ–¹æ³•æµ‹è¯•
        result = await crawler._process_and_store_content(
            "https://test.example.com/doc", 
            test_markdown
        )
        
        print(f"å¤„ç†ç»“æœ: {result}")
        assert result["success"], "å•é¡µé¢å¤„ç†åº”è¯¥æˆåŠŸ"
        assert result["chunks_stored"] == 2, "åº”è¯¥å­˜å‚¨2ä¸ªchunks"
        
        # éªŒè¯æ•°æ®åº“ä¸­çš„æ•°æ®
        async with PostgreSQLClient() as client:
            
            # æŸ¥è¯¢å­˜å‚¨çš„chunks
            stored_data = await client.execute_query("""
                SELECT id, url, content FROM crawled_pages
                WHERE url = 'https://test.example.com/doc'
                ORDER BY created_at
            """)
            
            print(f"å­˜å‚¨çš„chunksæ•°é‡: {len(stored_data)}")
            for i, data in enumerate(stored_data):
                print(f"  Chunk {i}: {data['url']}")
                print(f"    UUID: {data['id']}")
                print(f"    å†…å®¹é•¿åº¦: {len(data['content'])} å­—ç¬¦")
                print(f"    å†…å®¹é¢„è§ˆ: {data['content'][:50]}...")

            assert len(stored_data) == 2, "æ•°æ®åº“ä¸­åº”è¯¥æœ‰2ä¸ªchunkè®°å½•"
            assert all(data['url'] == 'https://test.example.com/doc' for data in stored_data), "æ‰€æœ‰è®°å½•åº”è¯¥ä½¿ç”¨åŸå§‹URL"
            assert all(data['id'] for data in stored_data), "æ‰€æœ‰è®°å½•åº”è¯¥æœ‰UUID"
    
    print("âœ… å•é¡µé¢åˆ†å—å­˜å‚¨æµ‹è¯•é€šè¿‡\n")


async def test_batch_processing():
    """æµ‹è¯•æ‰¹é‡å¤„ç†åŠŸèƒ½"""
    print("ğŸ§ª æµ‹è¯•æ‰¹é‡å¤„ç†åŠŸèƒ½...")
    
    # æ¨¡æ‹Ÿæ‰¹é‡çˆ¬å–ç»“æœ
    crawl_results = [
        {
            "url": "https://test.example.com/page1",
            "markdown": """# Page 1 Title
Content for page 1.

## Overview
Overview for page 1.

## Section A
Section A content."""
        },
        {
            "url": "https://test.example.com/page2", 
            "markdown": """# Page 2 Title
Content for page 2.

## Overview
Overview for page 2.

## Section B
Section B content.

## Section C
Section C content."""
        }
    ]
    
    async with IndependentCrawler() as crawler:
        # æµ‹è¯•æ‰¹é‡å¤„ç†
        result = await crawler._process_and_store_batch(crawl_results, "test_batch")
        
        print(f"æ‰¹é‡å¤„ç†ç»“æœ: {result}")
        assert result["success"], "æ‰¹é‡å¤„ç†åº”è¯¥æˆåŠŸ"
        assert result["total_pages"] == 2, "åº”è¯¥å¤„ç†2ä¸ªé¡µé¢"
        assert result["total_chunks"] == 3, "åº”è¯¥ç”Ÿæˆ3ä¸ªchunks (page1: 1ä¸ª, page2: 2ä¸ª)"
        
        # éªŒè¯æ•°æ®åº“ä¸­çš„æ•°æ®
        async with PostgreSQLClient() as client:
            stored_data = await client.execute_query("""
                SELECT id, url, content FROM crawled_pages
                WHERE url LIKE 'https://test.example.com/page%'
                ORDER BY url, created_at
            """)
            
            print(f"æ‰¹é‡å­˜å‚¨çš„chunksæ•°é‡: {len(stored_data)}")
            page1_chunks = [d for d in stored_data if 'page1' in d['url']]
            page2_chunks = [d for d in stored_data if 'page2' in d['url']]

            print(f"  Page1 chunks: {len(page1_chunks)}")
            print(f"  Page2 chunks: {len(page2_chunks)}")

            assert len(page1_chunks) == 1, "Page1åº”è¯¥æœ‰1ä¸ªchunk"
            assert len(page2_chunks) == 2, "Page2åº”è¯¥æœ‰2ä¸ªchunks"

            # éªŒè¯ä½¿ç”¨åŸå§‹URLå’ŒUUID
            for data in stored_data:
                assert '#chunk' not in data['url'], f"URLä¸åº”è¯¥åŒ…å«#chunk: {data['url']}"
                assert data['id'], f"è®°å½•åº”è¯¥æœ‰UUID: {data}"
    
    print("âœ… æ‰¹é‡å¤„ç†åŠŸèƒ½æµ‹è¯•é€šè¿‡\n")


async def test_embedding_generation():
    """æµ‹è¯•embeddingç”Ÿæˆ"""
    print("ğŸ§ª æµ‹è¯•embeddingç”Ÿæˆ...")
    
    async with PostgreSQLClient() as client:
        # æŸ¥è¯¢æ‰€æœ‰å­˜å‚¨çš„æ•°æ®
        stored_data = await client.execute_query("""
            SELECT id, url, embedding FROM crawled_pages
            WHERE url LIKE 'https://test.example.com/%'
            AND embedding IS NOT NULL
        """)
        
        print(f"æœ‰embeddingçš„è®°å½•æ•°é‡: {len(stored_data)}")
        
        for data in stored_data:
            # éªŒè¯embeddingä¸ä¸ºç©ºä¸”ç»´åº¦æ­£ç¡®
            embedding_str = data['embedding']
            assert embedding_str, "Embeddingä¸åº”è¯¥ä¸ºç©º"
            
            # è§£æembeddingå­—ç¬¦ä¸²ä¸ºåˆ—è¡¨
            import ast
            embedding = ast.literal_eval(embedding_str)
            assert len(embedding) == 2560, f"Embeddingç»´åº¦åº”è¯¥æ˜¯2560ï¼Œå®é™…æ˜¯{len(embedding)}"
            
            print(f"  {data['url']} (UUID: {str(data['id'])[:8]}...): embeddingç»´åº¦ {len(embedding)}")

            # éªŒè¯ä½¿ç”¨åŸå§‹URLå’ŒUUID
            assert '#chunk' not in data['url'], f"URLä¸åº”è¯¥åŒ…å«#chunk: {data['url']}"
            assert data['id'], f"è®°å½•åº”è¯¥æœ‰UUID: {data}"
    
    print("âœ… Embeddingç”Ÿæˆæµ‹è¯•é€šè¿‡\n")


async def main():
    """è¿è¡Œæ‰€æœ‰é›†æˆæµ‹è¯•"""
    print("ğŸš€ å¼€å§‹çˆ¬è™«é›†æˆæµ‹è¯•")
    print("=" * 50)
    
    try:
        await test_single_page_chunking()
        await test_batch_processing() 
        await test_embedding_generation()
        
        print("ğŸ‰ æ‰€æœ‰é›†æˆæµ‹è¯•é€šè¿‡ï¼")
        print("âœ… ä¿®å¤åçš„çˆ¬è™«åŠŸèƒ½å·¥ä½œæ­£å¸¸")
        
    except Exception as e:
        print(f"âŒ é›†æˆæµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return False
    
    return True


if __name__ == "__main__":
    success = asyncio.run(main())
    sys.exit(0 if success else 1)
