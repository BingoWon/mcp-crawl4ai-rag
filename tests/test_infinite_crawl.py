#!/usr/bin/env python3
"""
æµ‹è¯•æ— é™çˆ¬å–é€»è¾‘ - çŸ­æ—¶é—´è¿è¡ŒéªŒè¯
"""

import asyncio
import sys
from pathlib import Path

# æ·»åŠ srcç›®å½•åˆ°è·¯å¾„
sys.path.insert(0, str(Path(__file__).parent.parent / "src"))

from database.client import PostgreSQLClient
from crawler.core import IndependentCrawler


async def test_short_infinite_crawl():
    """æµ‹è¯•çŸ­æ—¶é—´çš„æ— é™çˆ¬å–"""
    print("ğŸ§ª æµ‹è¯•æ— é™çˆ¬å–é€»è¾‘ï¼ˆ5æ¬¡å¾ªç¯ååœæ­¢ï¼‰")
    
    # æ¸…ç©ºæ•°æ®åº“
    async with PostgreSQLClient() as client:
        await client.execute_command("DELETE FROM chunks")
        await client.execute_command("DELETE FROM pages")
        print("æ•°æ®åº“å·²æ¸…ç©º")
    
    # å¼€å§‹çˆ¬å–
    async with IndependentCrawler() as crawler:
        # ä¿®æ”¹çˆ¬å–æ–¹æ³•ä»¥æ”¯æŒé™åˆ¶æ¬¡æ•°
        start_url = "https://developer.apple.com/documentation/"
        
        if not start_url.startswith(crawler.APPLE_DOCS_URL_PREFIX):
            print("âŒ URLä¸æ”¯æŒ")
            return

        # Insert start URL if not exists
        await crawler.db_operations.insert_url_if_not_exists(start_url)
        print(f"å¼€å§‹æ— é™çˆ¬å–: {start_url}")

        crawl_count = 0
        max_crawls = 3  # é™åˆ¶çˆ¬å–æ¬¡æ•°ç”¨äºæµ‹è¯•
        
        while crawl_count < max_crawls:
            try:
                # Get next URL and content to crawl (minimum crawl_count)
                result = await crawler.db_operations.get_next_crawl_url()
                if not result:
                    print("æ²¡æœ‰URLå¯çˆ¬å–")
                    break

                next_url, existing_content = result
                crawl_count += 1
                print(f"\n=== çˆ¬å– #{crawl_count}: {next_url} ===")

                # Crawl and process the URL
                await crawler._crawl_and_process_url(next_url, bool(existing_content))
                
                # æ˜¾ç¤ºå½“å‰çŠ¶æ€
                async with PostgreSQLClient() as client:
                    pages_count = await client.fetch_val("SELECT COUNT(*) FROM pages")
                    chunks_count = await client.fetch_val("SELECT COUNT(*) FROM chunks")
                    print(f"å½“å‰çŠ¶æ€: {pages_count} é¡µé¢, {chunks_count} å—")
                    
                    # æ˜¾ç¤ºçˆ¬å–æ¬¡æ•°åˆ†å¸ƒ
                    crawl_stats = await client.execute_query("""
                        SELECT crawl_count, COUNT(*) as count 
                        FROM pages 
                        GROUP BY crawl_count 
                        ORDER BY crawl_count
                    """)
                    print("çˆ¬å–æ¬¡æ•°åˆ†å¸ƒ:", dict((row['crawl_count'], row['count']) for row in crawl_stats))

            except Exception as e:
                print(f"çˆ¬å–é”™è¯¯: {e}")
                break
        
        print(f"\nâœ… æµ‹è¯•å®Œæˆï¼Œå…±çˆ¬å– {crawl_count} æ¬¡")


async def main():
    """ä¸»æµ‹è¯•å‡½æ•°"""
    print("ğŸš€ å¼€å§‹æµ‹è¯•æ— é™çˆ¬å–é€»è¾‘")
    print("=" * 50)
    
    try:
        await test_short_infinite_crawl()
        
        # æ˜¾ç¤ºæœ€ç»ˆç»Ÿè®¡
        async with PostgreSQLClient() as client:
            print("\nğŸ“Š æœ€ç»ˆç»Ÿè®¡:")
            pages = await client.execute_query("""
                SELECT url, crawl_count, 
                       LENGTH(content) as content_length,
                       created_at, updated_at
                FROM pages 
                ORDER BY crawl_count DESC, created_at
            """)
            
            for page in pages:
                print(f"  {page['url']}")
                print(f"    çˆ¬å–æ¬¡æ•°: {page['crawl_count']}")
                print(f"    å†…å®¹é•¿åº¦: {page['content_length']} å­—ç¬¦")
                print(f"    åˆ›å»º: {page['created_at']}")
                print(f"    æ›´æ–°: {page['updated_at']}")
                print()
        
        print("ğŸ‰ æµ‹è¯•å®Œæˆ!")
        
    except Exception as e:
        print(f"\nâŒ æµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()


if __name__ == "__main__":
    asyncio.run(main())
