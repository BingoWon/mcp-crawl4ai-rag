#!/usr/bin/env python3
"""
åˆ†å¸ƒå¼çˆ¬è™«å®‰å…¨æ€§æµ‹è¯• - PostgreSQL FOR UPDATE SKIP LOCKED éªŒè¯
"""

import asyncio
import sys
from pathlib import Path
from dotenv import load_dotenv

load_dotenv()
src_path = Path(__file__).parent.parent / "src"
sys.path.insert(0, str(src_path))

from database import get_database_client, DatabaseOperations


async def test_realistic_crawling_scenario():
    """æµ‹è¯•çœŸå®çˆ¬è™«åœºæ™¯ä¸‹çš„URLåˆ†é…"""
    client = await get_database_client()
    db_ops = DatabaseOperations(client)

    all_urls = set()
    duplicate_count = 0

    async def crawler_worker(worker_id: int):
        """æ¨¡æ‹ŸçœŸå®çˆ¬è™«workerçš„è¡Œä¸º"""
        nonlocal duplicate_count

        # è·å–URLæ‰¹æ¬¡
        urls = await db_ops.get_urls_batch(2)

        # æ£€æŸ¥é‡å¤
        worker_urls = []
        for url in urls:
            if url in all_urls:
                duplicate_count += 1
                print(f"âŒ Duplicate: {url} (Worker {worker_id})")
            else:
                all_urls.add(url)
                worker_urls.append(url)

        # æ¨¡æ‹Ÿçˆ¬å–å¤„ç†æ—¶é—´
        if worker_urls:
            await asyncio.sleep(0.01)  # 10mså¤„ç†æ—¶é—´

            # æ¨¡æ‹Ÿæ›´æ–°crawl_countï¼ˆçœŸå®åœºæ™¯ä¸­ä¼šåšçš„ï¼‰
            for url in worker_urls:
                await db_ops.client.execute_command("""
                    UPDATE pages SET crawl_count = crawl_count + 1
                    WHERE url = $1
                """, url)

        return worker_urls

    # 3ä¸ªworkerï¼Œæ¯ä¸ªæ‰§è¡Œ2è½®
    all_results = []
    for round_num in range(2):
        print(f"Round {round_num + 1}:")
        round_results = await asyncio.gather(*[
            crawler_worker(i) for i in range(3)
        ])
        all_results.extend(round_results)
        await asyncio.sleep(0.05)  # è½®æ¬¡é—´éš”

    total_urls = sum(len(urls) for urls in all_results)
    unique_urls = len(all_urls)

    print(f"ğŸ“Š Total: {total_urls}, Unique: {unique_urls}, Duplicates: {duplicate_count}")
    return duplicate_count == 0


async def main():
    """ä¸»æµ‹è¯•å‡½æ•°"""
    print("ğŸ§ª Testing distributed crawling safety with PostgreSQL Advisory Locks")
    print("=" * 65)

    try:
        success = await test_realistic_crawling_scenario()

        if success:
            print("âœ… Test passed! No duplicate URLs detected.")
            print("ğŸ‰ Multi-machine crawling is now safe!")
        else:
            print("âŒ Test failed! Duplicate URLs detected.")

    except Exception as e:
        print(f"âŒ Test failed: {e}")
        import traceback
        traceback.print_exc()


if __name__ == "__main__":
    asyncio.run(main())
