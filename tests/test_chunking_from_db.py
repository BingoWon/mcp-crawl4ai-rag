#!/usr/bin/env python3
"""
æ•°æ®åº“Chunkingæµ‹è¯•è„šæœ¬

ä»PostgreSQLçš„pagesè¡¨ä¸­éšæœºé€‰å–ä¸€ä¸ªpageçš„contentè¿›è¡Œchunkingæµ‹è¯•ï¼Œ
ç”Ÿæˆè¯¦ç»†çš„æ•ˆæœæŠ¥å‘Šã€‚
"""

import sys
import json
import asyncio
from datetime import datetime
from pathlib import Path
from dotenv import load_dotenv

# Load environment variables from .env file
load_dotenv()

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root / 'src'))

# å¯¼å…¥å¿…è¦çš„æ¨¡å—
from chunking.chunker import SmartChunker
from database.client import create_database_client

async def get_largest_pages_content(count: int = 100):
    """ä»pagesè¡¨ä¸­è·å–contentæœ€å¤§çš„å‰Nä¸ªpages"""
    client = create_database_client()

    try:
        # åˆå§‹åŒ–æ•°æ®åº“è¿æ¥
        await client.initialize()

        # æŸ¥è¯¢contentæœ€å¤§çš„å‰Nä¸ªpages
        results = await client.fetch_all("""
            SELECT url, content
            FROM pages
            WHERE content IS NOT NULL
            AND content != ''
            AND LENGTH(content) > 1000
            AND (url = 'https://developer.apple.com/documentation'
                 OR url LIKE 'https://developer.apple.com/documentation/%')
            ORDER BY LENGTH(content) DESC
            LIMIT $1
        """, count)

        if results:
            pages = []
            for result in results:
                pages.append({
                    'url': result['url'],
                    'content': result['content']
                })
            return pages
        else:
            print("âŒ æ²¡æœ‰æ‰¾åˆ°ç¬¦åˆæ¡ä»¶çš„pageè®°å½•")
            return []

    except Exception as e:
        print(f"âŒ æ•°æ®åº“æŸ¥è¯¢å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return []
    finally:
        await client.close()

def analyze_chunks(chunks):
    """åˆ†æchunksçš„ç»Ÿè®¡ä¿¡æ¯"""
    if not chunks:
        return {
            'total_chunks': 0,
            'total_size': 0,
            'avg_size': 0,
            'min_size': 0,
            'max_size': 0,
            'size_distribution': {}
        }
    
    sizes = [len(chunk) for chunk in chunks]
    total_size = sum(sizes)
    
    # å¤§å°åˆ†å¸ƒç»Ÿè®¡
    size_ranges = {
        '0-1000': 0,
        '1001-2000': 0,
        '2001-2500': 0,
        '2501-3000': 0,
        '3001+': 0
    }
    
    for size in sizes:
        if size <= 1000:
            size_ranges['0-1000'] += 1
        elif size <= 2000:
            size_ranges['1001-2000'] += 1
        elif size <= 2500:
            size_ranges['2001-2500'] += 1
        elif size <= 3000:
            size_ranges['2501-3000'] += 1
        else:
            size_ranges['3001+'] += 1
    
    return {
        'total_chunks': len(chunks),
        'total_size': total_size,
        'avg_size': total_size // len(chunks),
        'min_size': min(sizes),
        'max_size': max(sizes),
        'size_distribution': size_ranges
    }

def analyze_context_inheritance(chunks):
    """åˆ†æcontextç»§æ‰¿æƒ…å†µ"""
    context_info = []
    
    for i, chunk in enumerate(chunks):
        try:
            data = json.loads(chunk)
            context = data.get('context', '')
            content = data.get('content', '')
            
            context_info.append({
                'chunk_id': i + 1,
                'context_length': len(context),
                'content_length': len(content),
                'context_preview': context[:100] if context else '',
                'content_preview': content[:100] if content else ''
            })
        except json.JSONDecodeError:
            context_info.append({
                'chunk_id': i + 1,
                'context_length': 0,
                'content_length': len(chunk),
                'context_preview': '',
                'content_preview': chunk[:100]
            })
    
    return context_info

def generate_batch_report(all_results, batch_stats, output_file):
    """ç”Ÿæˆæ‰¹é‡æµ‹è¯•çš„è¯¦ç»†æŠ¥å‘Š"""
    timestamp = datetime.now().strftime("%Y-%m-%d %H:%M:%S")

    report_lines = [
        "=" * 80,
        "Appleæ–‡æ¡£æ™ºèƒ½åˆ†å—å™¨ - æ•°æ®åº“å¤§è§„æ¨¡æµ‹è¯•æŠ¥å‘Šï¼ˆå‰100æœ€å¤§æ–‡æ¡£ï¼‰",
        "=" * 80,
        f"ç”Ÿæˆæ—¶é—´: {timestamp}",
        f"æµ‹è¯•ç­–ç•¥: æ–°æ™ºèƒ½åˆ†å—å™¨ï¼ˆåŠ¨æ€è‡ªé€‚åº”ç­–ç•¥ï¼‰",
        f"æ•°æ®æ¥æº: PostgreSQLæ•°æ®åº“æŒ‰contentå¤§å°é™åºæ’åˆ—",
        "",
        "ğŸ“Š æ‰¹é‡æµ‹è¯•ç»Ÿè®¡æ‘˜è¦",
        "-" * 40,
        f"æµ‹è¯•é¡µé¢æ€»æ•°: {batch_stats['total_pages']}",
        f"ç”Ÿæˆchunksæ€»æ•°: {batch_stats['total_chunks']}",
        f"å¹³å‡æ¯é¡µchunks: {batch_stats['avg_chunks_per_page']:.1f}",
        "",
        "ğŸ“„ åŸå§‹å†…å®¹ç»Ÿè®¡",
        "-" * 40,
        f"æ€»åŸå§‹å†…å®¹: {batch_stats['total_original_size']:,} å­—ç¬¦",
        f"å¹³å‡é¡µé¢å¤§å°: {batch_stats['avg_original_size']:,} å­—ç¬¦",
        f"æœ€å°é¡µé¢å¤§å°: {batch_stats['min_original_size']:,} å­—ç¬¦",
        f"æœ€å¤§é¡µé¢å¤§å°: {batch_stats['max_original_size']:,} å­—ç¬¦",
        "",
        "ğŸ“ˆ Chunkå¤§å°ç»Ÿè®¡",
        "-" * 40,
        f"æ€»chunkå¤§å°: {batch_stats['total_chunk_size']:,} å­—ç¬¦",
        f"å¹³å‡chunkå¤§å°: {batch_stats['avg_chunk_size']:,} å­—ç¬¦",
        f"æœ€å°chunkå¤§å°: {batch_stats['min_chunk_size']:,} å­—ç¬¦",
        f"æœ€å¤§chunkå¤§å°: {batch_stats['max_chunk_size']:,} å­—ç¬¦",
        "",
        "ğŸ“Š Chunkå¤§å°åˆ†å¸ƒ",
        "-" * 40,
    ]

    # æ·»åŠ å¤§å°åˆ†å¸ƒç»Ÿè®¡
    total_chunks = batch_stats['total_chunks']
    for size_range, count in batch_stats['size_distribution'].items():
        percentage = (count / total_chunks * 100) if total_chunks > 0 else 0
        report_lines.append(f"{size_range:>12}: {count:>4} chunks ({percentage:>5.1f}%)")

    report_lines.extend([
        "",
        "ğŸ“ è¯¦ç»†é¡µé¢åˆ†æ",
        "-" * 40,
    ])

    # æ·»åŠ æ¯ä¸ªé¡µé¢çš„è¯¦ç»†åˆ†æ
    for i, result in enumerate(all_results, 1):
        report_lines.extend([
            f"é¡µé¢ {i}: {result['url']}",
            f"  åŸå§‹å¤§å°: {result['original_size']:,} å­—ç¬¦",
            f"  ç”Ÿæˆchunks: {result['chunk_count']} ä¸ª",
            f"  Chunkå¤§å°: {min(result['chunk_sizes'])}-{max(result['chunk_sizes'])} å­—ç¬¦",
            f"  å¹³å‡å¤§å°: {sum(result['chunk_sizes'])//len(result['chunk_sizes'])} å­—ç¬¦",
            ""
        ])

    report_lines.extend([
        "ğŸ” Context/Contentåˆ†æç¤ºä¾‹",
        "-" * 40,
    ])

    # æ·»åŠ å‰3ä¸ªé¡µé¢çš„Contextåˆ†æç¤ºä¾‹
    for i, result in enumerate(all_results[:3], 1):
        report_lines.extend([
            f"=== é¡µé¢ {i} Contextåˆ†æ ===",
            f"URL: {result['url']}",
        ])

        for j, info in enumerate(result['context_info'][:2], 1):  # åªæ˜¾ç¤ºå‰2ä¸ªchunks
            report_lines.extend([
                f"  Chunk {j}:",
                f"    Contexté•¿åº¦: {info['context_length']} å­—ç¬¦",
                f"    Contenté•¿åº¦: {info['content_length']} å­—ç¬¦",
                f"    Contexté¢„è§ˆ: {info['context_preview']}",
                f"    Contenté¢„è§ˆ: {info['content_preview']}",
                ""
            ])

    report_lines.append("=" * 80)

    # å†™å…¥æ–‡ä»¶
    with open(output_file, 'w', encoding='utf-8') as f:
        f.write('\n'.join(report_lines))

def generate_report(page_info, chunks, stats, context_info, output_file):
    """ç”Ÿæˆè¯¦ç»†çš„æµ‹è¯•æŠ¥å‘Š"""
    timestamp = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
    
    report_lines = [
        "=" * 80,
        "å¤šçº§é€’è¿›Chunkingæµ‹è¯•æŠ¥å‘Š",
        "=" * 80,
        f"ç”Ÿæˆæ—¶é—´: {timestamp}",
        f"é¡µé¢URL: {page_info['url']}",
        "",
        "ğŸ“Š åŸå§‹å†…å®¹ä¿¡æ¯",
        "-" * 40,
        f"åŸå§‹å†…å®¹é•¿åº¦: {len(page_info['content'])} å­—ç¬¦",
        f"åŸå§‹å†…å®¹é¢„è§ˆ: {page_info['content'][:200]}...",
        "",
        "ğŸ“ˆ Chunkingç»Ÿè®¡ç»“æœ",
        "-" * 40,
        f"ç”Ÿæˆchunksæ•°é‡: {stats['total_chunks']}",
        f"æ€»å­—ç¬¦æ•°: {stats['total_size']}",
        f"å¹³å‡chunkå¤§å°: {stats['avg_size']} å­—ç¬¦",
        f"æœ€å°chunkå¤§å°: {stats['min_size']} å­—ç¬¦",
        f"æœ€å¤§chunkå¤§å°: {stats['max_size']} å­—ç¬¦",
        "",
        "ğŸ“Š å¤§å°åˆ†å¸ƒç»Ÿè®¡",
        "-" * 40,
    ]
    
    for size_range, count in stats['size_distribution'].items():
        percentage = (count / stats['total_chunks'] * 100) if stats['total_chunks'] > 0 else 0
        report_lines.append(f"{size_range:>12}: {count:>3} chunks ({percentage:>5.1f}%)")
    
    report_lines.extend([
        "",
        "ğŸ” Contextç»§æ‰¿åˆ†æ",
        "-" * 40,
    ])
    
    for info in context_info:
        report_lines.extend([
            f"Chunk {info['chunk_id']}:",
            f"  Contexté•¿åº¦: {info['context_length']} å­—ç¬¦",
            f"  Contenté•¿åº¦: {info['content_length']} å­—ç¬¦",
            f"  Contexté¢„è§ˆ: {info['context_preview']}",
            f"  Contenté¢„è§ˆ: {info['content_preview']}",
            ""
        ])
    
    report_lines.extend([
        "ğŸ“ è¯¦ç»†Chunkså†…å®¹",
        "-" * 40,
    ])
    
    for i, chunk in enumerate(chunks):
        report_lines.extend([
            f"=== Chunk {i+1} ({len(chunk)} å­—ç¬¦) ===",
            chunk,
            ""
        ])
    
    report_lines.append("=" * 80)
    
    # å†™å…¥æ–‡ä»¶
    with open(output_file, 'w', encoding='utf-8') as f:
        f.write('\n'.join(report_lines))

def analyze_batch_results(all_results):
    """åˆ†ææ‰¹é‡æµ‹è¯•ç»“æœ"""
    if not all_results:
        return {}

    # æ”¶é›†æ‰€æœ‰ç»Ÿè®¡æ•°æ®
    all_chunk_sizes = []
    all_original_sizes = []
    total_pages = len(all_results)
    total_chunks = 0

    for result in all_results:
        all_original_sizes.append(result['original_size'])
        all_chunk_sizes.extend(result['chunk_sizes'])
        total_chunks += result['chunk_count']

    # è®¡ç®—å…¨å±€ç»Ÿè®¡
    batch_stats = {
        'total_pages': total_pages,
        'total_chunks': total_chunks,
        'avg_chunks_per_page': total_chunks / total_pages,
        'total_original_size': sum(all_original_sizes),
        'avg_original_size': sum(all_original_sizes) // total_pages,
        'min_original_size': min(all_original_sizes),
        'max_original_size': max(all_original_sizes),
        'total_chunk_size': sum(all_chunk_sizes),
        'avg_chunk_size': sum(all_chunk_sizes) // len(all_chunk_sizes) if all_chunk_sizes else 0,
        'min_chunk_size': min(all_chunk_sizes) if all_chunk_sizes else 0,
        'max_chunk_size': max(all_chunk_sizes) if all_chunk_sizes else 0,
    }

    # å¤§å°åˆ†å¸ƒç»Ÿè®¡
    size_ranges = {
        '0-1000': 0,
        '1001-2000': 0,
        '2001-3000': 0,
        '3001-4000': 0,
        '4001-5000': 0,
        '5001+': 0
    }

    for size in all_chunk_sizes:
        if size <= 1000:
            size_ranges['0-1000'] += 1
        elif size <= 2000:
            size_ranges['1001-2000'] += 1
        elif size <= 3000:
            size_ranges['2001-3000'] += 1
        elif size <= 4000:
            size_ranges['3001-4000'] += 1
        elif size <= 5000:
            size_ranges['4001-5000'] += 1
        else:
            size_ranges['5001+'] += 1

    batch_stats['size_distribution'] = size_ranges
    return batch_stats

async def main():
    """ä¸»å‡½æ•° - æ‰¹é‡æµ‹è¯•"""
    print("ğŸš€ å¼€å§‹æ•°æ®åº“æ‰¹é‡Chunkingæµ‹è¯•...")

    # 1. è·å–æœ€å¤§contentçš„é¡µé¢
    test_count = 1000  # æµ‹è¯•100ä¸ªé¡µé¢
    print(f"ğŸ“Š ä»æ•°æ®åº“è·å–contentæœ€å¤§çš„å‰ {test_count} ä¸ªé¡µé¢...")
    pages = await get_largest_pages_content(test_count)
    if not pages:
        print("âŒ æ— æ³•è·å–é¡µé¢å†…å®¹ï¼Œæµ‹è¯•ç»ˆæ­¢")
        return

    print(f"âœ… è·å–åˆ° {len(pages)} ä¸ªé¡µé¢")

    # 2. æ‰¹é‡æ‰§è¡Œchunking
    print("ğŸ”§ æ‰§è¡Œæ‰¹é‡æ™ºèƒ½chunking...")
    chunker = SmartChunker()
    all_results = []

    for i, page in enumerate(pages, 1):
        print(f"å¤„ç†é¡µé¢ {i}/{len(pages)}: {page['url']}...")

        chunks = chunker.chunk_text(page['content'])
        stats = analyze_chunks(chunks)
        context_info = analyze_context_inheritance(chunks)

        # æ”¶é›†ç»“æœ
        result = {
            'url': page['url'],
            'original_size': len(page['content']),
            'chunk_count': len(chunks),
            'chunk_sizes': [len(chunk) for chunk in chunks],
            'chunks': chunks,
            'stats': stats,
            'context_info': context_info
        }
        all_results.append(result)

    print(f"âœ… æ‰¹é‡Chunkingå®Œæˆ: å¤„ç†äº† {len(all_results)} ä¸ªé¡µé¢")

    # 3. åˆ†ææ‰¹é‡ç»“æœ
    print("ğŸ“ˆ åˆ†ææ‰¹é‡ç»“æœ...")
    batch_stats = analyze_batch_results(all_results)

    # 4. ç”Ÿæˆè¯¦ç»†æŠ¥å‘Š
    output_file = project_root / 'tests' / f'chunking_batch_report_{datetime.now().strftime("%Y%m%d_%H%M%S")}.txt'
    print(f"ğŸ“ ç”Ÿæˆè¯¦ç»†æŠ¥å‘Š: {output_file}")

    generate_batch_report(all_results, batch_stats, output_file)

    print("ğŸ‰ æ‰¹é‡æµ‹è¯•å®Œæˆï¼")
    print(f"ğŸ“Š æ‰¹é‡ç»Ÿè®¡æ‘˜è¦:")
    print(f"   - æµ‹è¯•é¡µé¢æ•°: {batch_stats['total_pages']}")
    print(f"   - æ€»chunksæ•°: {batch_stats['total_chunks']}")
    print(f"   - å¹³å‡æ¯é¡µchunks: {batch_stats['avg_chunks_per_page']:.1f}")
    print(f"   - å¹³å‡chunkå¤§å°: {batch_stats['avg_chunk_size']} å­—ç¬¦")
    print(f"   - Chunkå¤§å°èŒƒå›´: {batch_stats['min_chunk_size']} - {batch_stats['max_chunk_size']} å­—ç¬¦")
    print(f"ğŸ“„ è¯¦ç»†æŠ¥å‘Š: {output_file}")

if __name__ == "__main__":
    asyncio.run(main())
