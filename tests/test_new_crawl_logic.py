#!/usr/bin/env python3
"""
æµ‹è¯•æ–°çš„æ•°æ®åº“é©±åŠ¨çˆ¬å–é€»è¾‘
"""

import asyncio
import sys
from pathlib import Path

# æ·»åŠ srcç›®å½•åˆ°è·¯å¾„
sys.path.insert(0, str(Path(__file__).parent.parent / "src"))

from database.client import PostgreSQLClient
from database.operations import DatabaseOperations
from crawler.core import IndependentCrawler


async def test_database_operations():
    """æµ‹è¯•æ–°çš„æ•°æ®åº“æ“ä½œæ–¹æ³•"""
    print("ğŸ§ª æµ‹è¯•æ•°æ®åº“æ“ä½œæ–¹æ³•")
    
    async with PostgreSQLClient() as client:
        db_ops = DatabaseOperations(client)
        
        # æµ‹è¯•æ’å…¥URL
        test_url = "https://developer.apple.com/documentation/test"
        inserted = await db_ops.insert_url_if_not_exists(test_url)
        print(f"æ’å…¥æ–°URL: {inserted}")
        
        # å†æ¬¡æ’å…¥ç›¸åŒURLï¼ˆåº”è¯¥è¿”å›Falseï¼‰
        inserted_again = await db_ops.insert_url_if_not_exists(test_url)
        print(f"é‡å¤æ’å…¥URL: {inserted_again}")
        
        # è·å–ä¸‹ä¸€ä¸ªçˆ¬å–URL
        next_url = await db_ops.get_next_crawl_url()
        print(f"ä¸‹ä¸€ä¸ªçˆ¬å–URL: {next_url}")
        
        # æ›´æ–°é¡µé¢å†…å®¹
        await db_ops.update_page_after_crawl(test_url, "æµ‹è¯•å†…å®¹")
        print("æ›´æ–°é¡µé¢å†…å®¹å®Œæˆ")
        
        # å†æ¬¡è·å–ä¸‹ä¸€ä¸ªURLï¼ˆcrawl_countåº”è¯¥å¢åŠ äº†ï¼‰
        next_url_after = await db_ops.get_next_crawl_url()
        print(f"æ›´æ–°åçš„ä¸‹ä¸€ä¸ªURL: {next_url_after}")
        
        # æŸ¥çœ‹pagesè¡¨æ•°æ®
        pages = await client.execute_query("SELECT url, crawl_count, content FROM pages")
        print(f"Pagesè¡¨æ•°æ®: {pages}")


async def test_crawler_initialization():
    """æµ‹è¯•çˆ¬è™«åˆå§‹åŒ–"""
    print("\nğŸ§ª æµ‹è¯•çˆ¬è™«åˆå§‹åŒ–")
    
    try:
        async with IndependentCrawler() as crawler:
            print("âœ… çˆ¬è™«åˆå§‹åŒ–æˆåŠŸ")
            
            # æµ‹è¯•URLæ¸…æ´—
            test_urls = [
                "https://developer.apple.com/documentation/test#section",
                "https://developer.apple.com/documentation/test/",
                "https://developer.apple.com/documentation/test"
            ]
            
            for url in test_urls:
                clean_url = crawler.clean_and_normalize_url(url)
                print(f"åŸURL: {url} -> æ¸…æ´—å: {clean_url}")
                
    except Exception as e:
        print(f"âŒ çˆ¬è™«åˆå§‹åŒ–å¤±è´¥: {e}")


async def test_link_storage():
    """æµ‹è¯•é“¾æ¥å­˜å‚¨é€»è¾‘"""
    print("\nğŸ§ª æµ‹è¯•é“¾æ¥å­˜å‚¨é€»è¾‘")
    
    async with IndependentCrawler() as crawler:
        test_links = [
            "https://developer.apple.com/documentation/foundation",
            "https://developer.apple.com/documentation/uikit",
            "https://example.com/not-apple",  # åº”è¯¥è¢«è¿‡æ»¤
            "https://developer.apple.com/documentation/foundation#overview"  # åº”è¯¥è¢«æ¸…æ´—
        ]
        
        await crawler._store_discovered_links(test_links)
        print("é“¾æ¥å­˜å‚¨å®Œæˆ")
        
        # æŸ¥çœ‹å­˜å‚¨çš„é“¾æ¥
        async with PostgreSQLClient() as client:
            pages = await client.execute_query("SELECT url, crawl_count FROM pages ORDER BY created_at")
            print("å­˜å‚¨çš„é¡µé¢:")
            for page in pages:
                print(f"  {page['url']} (crawl_count: {page['crawl_count']})")


async def main():
    """ä¸»æµ‹è¯•å‡½æ•°"""
    print("ğŸš€ å¼€å§‹æµ‹è¯•æ–°çš„çˆ¬å–é€»è¾‘")
    print("=" * 50)
    
    try:
        await test_database_operations()
        await test_crawler_initialization()
        await test_link_storage()
        
        print("\nğŸ‰ æ‰€æœ‰æµ‹è¯•å®Œæˆ!")
        
    except Exception as e:
        print(f"\nâŒ æµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()


if __name__ == "__main__":
    asyncio.run(main())
