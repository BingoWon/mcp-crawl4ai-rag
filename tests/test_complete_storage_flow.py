#!/usr/bin/env python3
"""
æµ‹è¯•å®Œæ•´çš„å­˜å‚¨æµç¨‹ï¼špagesè¡¨ + chunksè¡¨
"""

import asyncio
import sys
from pathlib import Path

# æ·»åŠ srcç›®å½•åˆ°è·¯å¾„
sys.path.insert(0, str(Path(__file__).parent.parent / "src"))

from crawler.core import IndependentCrawler
from database.client import PostgreSQLClient


async def test_complete_storage_flow():
    """æµ‹è¯•å®Œæ•´çš„å­˜å‚¨æµç¨‹"""
    print("ğŸ§ª æµ‹è¯•å®Œæ•´çš„å­˜å‚¨æµç¨‹...")
    
    test_url = "https://test-complete-flow.example.com/doc"
    test_markdown = """# Complete Flow Test
This is a test document for complete storage flow.

## Overview
This document tests the complete storage flow from page to chunks.

## Section 1
Content for section 1 with some details.

## Section 2
Content for section 2 with more information."""

    # æ¸…ç†æµ‹è¯•æ•°æ®
    async with PostgreSQLClient() as client:
        await client.execute_query("DELETE FROM chunks WHERE url = $1", test_url)
        await client.execute_query("DELETE FROM pages WHERE url = $1", test_url)
        print("âœ… æµ‹è¯•æ•°æ®å·²æ¸…ç†")

    async with IndependentCrawler() as crawler:
        # æµ‹è¯•å•é¡µé¢å¤„ç†
        result = await crawler._process_and_store_content(test_url, test_markdown)
        
        print(f"çˆ¬è™«å¤„ç†ç»“æœ: {result}")
        assert result["success"], "çˆ¬è™«å¤„ç†åº”è¯¥æˆåŠŸ"
        
        # éªŒè¯pagesè¡¨ä¸­çš„æ•°æ®
        async with PostgreSQLClient() as client:
            pages_data = await client.execute_query("""
                SELECT id, url, content, created_at, updated_at FROM pages 
                WHERE url = $1
            """, test_url)
            
            print(f"\nğŸ“‹ pagesè¡¨ä¸­çš„è®°å½•æ•°: {len(pages_data)}")
            for page in pages_data:
                print(f"  é¡µé¢: {page['url']}")
                print(f"    å†…å®¹é•¿åº¦: {len(page['content'])} å­—ç¬¦")
                print(f"    åˆ›å»ºæ—¶é—´: {page['created_at']}")
                print(f"    æ›´æ–°æ—¶é—´: {page['updated_at']}")
            
            assert len(pages_data) == 1, "åº”è¯¥æœ‰1ä¸ªpageè®°å½•"
            assert pages_data[0]['content'] == test_markdown, "é¡µé¢å†…å®¹åº”è¯¥æ­£ç¡®"
            
            # éªŒè¯chunksè¡¨ä¸­çš„æ•°æ®
            chunks_data = await client.execute_query("""
                SELECT id, url, content, embedding FROM chunks 
                WHERE url = $1
                ORDER BY created_at
            """, test_url)
            
            print(f"\nğŸ“‹ chunksè¡¨ä¸­çš„è®°å½•æ•°: {len(chunks_data)}")
            for i, chunk in enumerate(chunks_data):
                print(f"  Chunk {i}: {chunk['url']}")
                print(f"    å†…å®¹é•¿åº¦: {len(chunk['content'])} å­—ç¬¦")
                print(f"    æœ‰embedding: {'æ˜¯' if chunk['embedding'] else 'å¦'}")
            
            assert len(chunks_data) >= 1, "åº”è¯¥è‡³å°‘æœ‰1ä¸ªchunkè®°å½•"
            assert all(chunk['embedding'] for chunk in chunks_data), "æ‰€æœ‰chunkéƒ½åº”è¯¥æœ‰embedding"
    
    print("âœ… å®Œæ•´å­˜å‚¨æµç¨‹æµ‹è¯•é€šè¿‡\n")


async def test_page_update_flow():
    """æµ‹è¯•é¡µé¢æ›´æ–°æµç¨‹"""
    print("ğŸ§ª æµ‹è¯•é¡µé¢æ›´æ–°æµç¨‹...")
    
    test_url = "https://test-update-flow.example.com/doc"
    original_content = """# Original Content
This is the original content."""
    
    updated_content = """# Updated Content
This is the updated content with more information."""

    async with IndependentCrawler() as crawler:
        # ç¬¬ä¸€æ¬¡å­˜å‚¨
        await crawler._process_and_store_content(test_url, original_content)
        
        # ç¬¬äºŒæ¬¡å­˜å‚¨ï¼ˆæ›´æ–°ï¼‰
        await crawler._process_and_store_content(test_url, updated_content)
        
        # éªŒè¯pagesè¡¨ä¸­åªæœ‰ä¸€æ¡è®°å½•ï¼Œä½†å†…å®¹å·²æ›´æ–°
        async with PostgreSQLClient() as client:
            pages_data = await client.execute_query("""
                SELECT id, url, content, created_at, updated_at FROM pages 
                WHERE url = $1
            """, test_url)
            
            print(f"pagesè¡¨ä¸­çš„è®°å½•æ•°: {len(pages_data)}")
            assert len(pages_data) == 1, "åº”è¯¥åªæœ‰1ä¸ªpageè®°å½•ï¼ˆå»é‡ï¼‰"
            assert pages_data[0]['content'] == updated_content, "å†…å®¹åº”è¯¥å·²æ›´æ–°"
            assert pages_data[0]['updated_at'] > pages_data[0]['created_at'], "æ›´æ–°æ—¶é—´åº”è¯¥å¤§äºåˆ›å»ºæ—¶é—´"
            
            print(f"  é¡µé¢: {pages_data[0]['url']}")
            print(f"    å†…å®¹: å·²æ›´æ–°ä¸ºæ–°å†…å®¹")
            print(f"    åˆ›å»ºæ—¶é—´: {pages_data[0]['created_at']}")
            print(f"    æ›´æ–°æ—¶é—´: {pages_data[0]['updated_at']}")
    
    print("âœ… é¡µé¢æ›´æ–°æµç¨‹æµ‹è¯•é€šè¿‡\n")


async def test_batch_storage_flow():
    """æµ‹è¯•æ‰¹é‡å­˜å‚¨æµç¨‹"""
    print("ğŸ§ª æµ‹è¯•æ‰¹é‡å­˜å‚¨æµç¨‹...")
    
    # æ¨¡æ‹Ÿæ‰¹é‡çˆ¬å–ç»“æœ
    crawl_results = [
        {
            'url': 'https://test-batch.example.com/page1',
            'markdown': '# Page 1\n## Overview\nContent for page 1.\n## Section 1\nMore content.'
        },
        {
            'url': 'https://test-batch.example.com/page2',
            'markdown': '# Page 2\n## Overview\nContent for page 2.\n## Section 1\nMore content.'
        }
    ]

    # æ¸…ç†æµ‹è¯•æ•°æ®
    async with PostgreSQLClient() as client:
        for result in crawl_results:
            await client.execute_query("DELETE FROM chunks WHERE url = $1", result['url'])
            await client.execute_query("DELETE FROM pages WHERE url = $1", result['url'])

    async with IndependentCrawler() as crawler:
        # æµ‹è¯•æ‰¹é‡å¤„ç†
        result = await crawler._process_and_store_batch(crawl_results, "test_batch")
        
        print(f"æ‰¹é‡å¤„ç†ç»“æœ: {result}")
        assert result["success"], "æ‰¹é‡å¤„ç†åº”è¯¥æˆåŠŸ"
        
        # éªŒè¯pagesè¡¨
        async with PostgreSQLClient() as client:
            pages_data = await client.execute_query("""
                SELECT url, content FROM pages 
                WHERE url LIKE 'https://test-batch.example.com/%'
                ORDER BY url
            """)
            
            print(f"\npagesè¡¨ä¸­çš„è®°å½•æ•°: {len(pages_data)}")
            assert len(pages_data) == 2, "åº”è¯¥æœ‰2ä¸ªpageè®°å½•"
            
            # éªŒè¯chunksè¡¨
            chunks_data = await client.execute_query("""
                SELECT url, content FROM chunks 
                WHERE url LIKE 'https://test-batch.example.com/%'
                ORDER BY url
            """)
            
            print(f"chunksè¡¨ä¸­çš„è®°å½•æ•°: {len(chunks_data)}")
            assert len(chunks_data) >= 2, "åº”è¯¥è‡³å°‘æœ‰2ä¸ªchunkè®°å½•"
    
    print("âœ… æ‰¹é‡å­˜å‚¨æµç¨‹æµ‹è¯•é€šè¿‡\n")


async def main():
    """è¿è¡Œæ‰€æœ‰å®Œæ•´å­˜å‚¨æµç¨‹æµ‹è¯•"""
    print("ğŸš€ å¼€å§‹å®Œæ•´å­˜å‚¨æµç¨‹æµ‹è¯•")
    print("=" * 50)
    
    try:
        await test_complete_storage_flow()
        await test_page_update_flow()
        await test_batch_storage_flow()
        
        print("ğŸ‰ æ‰€æœ‰å®Œæ•´å­˜å‚¨æµç¨‹æµ‹è¯•é€šè¿‡ï¼")
        print("âœ… pagesè¡¨å’Œchunksè¡¨çš„å®Œæ•´å­˜å‚¨æœºåˆ¶å·¥ä½œæ­£å¸¸")
        
    except Exception as e:
        print(f"âŒ å®Œæ•´å­˜å‚¨æµç¨‹æµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return False
    
    return True


if __name__ == "__main__":
    success = asyncio.run(main())
    sys.exit(0 if success else 1)
