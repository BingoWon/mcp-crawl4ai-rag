#!/usr/bin/env python3
"""
Database Viewer API
æ•°æ®åº“æŸ¥çœ‹å™¨API

æä¾›pageså’Œchunksæ•°æ®çš„æŸ¥è¯¢æ¥å£ï¼Œæ”¯æŒåˆ†é¡µã€æ’åºã€æœç´¢å’Œç»Ÿè®¡åŠŸèƒ½ã€‚

=== æ ¸å¿ƒåŠŸèƒ½ ===

**Pagesæ¥å£ (/api/pages)**
- åˆ†é¡µæŸ¥è¯¢ï¼šæ”¯æŒpageã€sizeå‚æ•°
- æ’åºåŠŸèƒ½ï¼šæ”¯æŒå¤šå­—æ®µæ’åºï¼ˆidã€urlã€crawl_countã€process_countã€created_atã€last_crawled_atï¼‰
- æœç´¢è¿‡æ»¤ï¼šæ”¯æŒURLå…³é”®è¯æœç´¢
- ç»Ÿè®¡ä¿¡æ¯ï¼šæä¾›çˆ¬å–é—´éš”æ—¶é—´ç»Ÿè®¡

**Chunksæ¥å£ (/api/chunks)**
- åˆ†é¡µæŸ¥è¯¢ï¼šæ”¯æŒpageã€sizeå‚æ•°
- å†…å®¹å±•ç¤ºï¼šæ˜¾ç¤ºchunkå†…å®¹å’Œç›¸å…³URLä¿¡æ¯

**ç»Ÿè®¡æ¥å£ (/api/stats)**
- é¡µé¢ç»Ÿè®¡ï¼šæ€»æ•°ã€æœ‰å†…å®¹é¡µé¢æ•°ã€å†…å®¹ç™¾åˆ†æ¯”
- å¤„ç†ç»Ÿè®¡ï¼šå¹³å‡çˆ¬å–æ¬¡æ•°ã€å¹³å‡å¤„ç†æ¬¡æ•°ï¼ˆä»…æœ‰å†…å®¹é¡µé¢ï¼‰
- ç²¾åº¦æ§åˆ¶ï¼šçˆ¬å–å’Œå¤„ç†æ¬¡æ•°ä¿ç•™4ä½å°æ•°ï¼Œç™¾åˆ†æ¯”ä¿ç•™2ä½å°æ•°

=== çˆ¬å–é—´éš”æ—¶é—´ç»Ÿè®¡ ===

**è®¡ç®—é€»è¾‘ï¼š**
- æ•°æ®è¿‡æ»¤ï¼šåªåŒ…å«crawl_count > 0çš„é¡µé¢ï¼ˆå·²å®é™…çˆ¬å–çš„é¡µé¢ï¼‰
- æ—¶é—´æ’åºï¼šæŒ‰last_crawled_atå‡åºæ’åˆ—ç¡®ä¿æ—¶é—´é¡ºåº
- é—´éš”è®¡ç®—ï¼šè®¡ç®—ç›¸é‚»è®°å½•çš„æ—¶é—´å·®ï¼ˆç§’ï¼‰
- å¹³å‡å€¼ï¼šæ‰€æœ‰æ—¶é—´é—´éš”çš„ç®—æœ¯å¹³å‡å€¼

**ä¸šåŠ¡ä»·å€¼ï¼š**
- æ’é™¤æœªçˆ¬å–é¡µé¢ï¼šcrawl_count = 0çš„é¡µé¢åªæ˜¯è®°å½•ç­‰å¾…çˆ¬å–ï¼Œä¸å‚ä¸ç»Ÿè®¡
- çœŸå®æ€§èƒ½æŒ‡æ ‡ï¼šåæ˜ å®é™…çˆ¬å–æ“ä½œçš„æ—¶é—´é—´éš”
- ç³»ç»Ÿç›‘æ§ï¼šå¸®åŠ©è¯„ä¼°çˆ¬å–é¢‘ç‡å’Œç³»ç»Ÿæ€§èƒ½
"""

import sys
from pathlib import Path
from fastapi import FastAPI
from fastapi.middleware.cors import CORSMiddleware
from fastapi.responses import JSONResponse
import uvicorn

# æ·»åŠ srcç›®å½•åˆ°è·¯å¾„
sys.path.insert(0, str(Path(__file__).parent.parent / "src"))

from database.utils import get_database_client

app = FastAPI(title="Database Viewer API", version="1.0.0")

# æ·»åŠ CORSä¸­é—´ä»¶
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)


@app.get("/api/pages")
async def get_pages(search: str = "", sort: str = "last_crawled_at", order: str = "desc") -> JSONResponse:
    """è·å–pagesè¡¨æ•°æ®ï¼ˆå›ºå®šå‰100æ¡ï¼‰"""
    try:
        client = await get_database_client()

        # æ„å»ºæŸ¥è¯¢æ¡ä»¶
        where_clause = ""
        params = []
        if search:
            where_clause = "WHERE url ILIKE $1 OR content ILIKE $1"
            params.append(f"%{search}%")

        # æ„å»ºæ’åº - ä¼˜é›…ç°ä»£ç²¾ç®€
        valid_sorts = ["id", "url", "crawl_count", "process_count", "created_at", "last_crawled_at"]
        sort_column = sort if sort in valid_sorts else "last_crawled_at"
        sort_order = "ASC" if order.lower() == "asc" else "DESC"

        # è·å–å‰100æ¡æ•°æ® - å›ºå®šæ•°é‡ï¼Œæ— åˆ†é¡µ
        if search:
            query = f"""
                SELECT id, url, content, crawl_count, process_count, created_at, last_crawled_at
                FROM pages {where_clause}
                ORDER BY {sort_column} {sort_order}
                LIMIT 100
            """
            pages = await client.fetch_all(query, params[0])
        else:
            query = f"""
                SELECT id, url, content, crawl_count, process_count, created_at, last_crawled_at
                FROM pages
                ORDER BY {sort_column} {sort_order}
                LIMIT 100
            """
            pages = await client.fetch_all(query)

        # è®¡ç®—å¹³å‡çˆ¬å–é—´éš”æ—¶é—´ï¼ˆåªåŒ…å«å·²çˆ¬å–çš„é¡µé¢ï¼‰- ä¼˜é›…ç°ä»£ç²¾ç®€
        avg_crawl_interval = None
        crawled_pages = [page for page in pages if page["crawl_count"] > 0]

        if len(crawled_pages) >= 2:
            # æŒ‰last_crawled_atæ’åºï¼ˆç¡®ä¿æ—¶é—´é¡ºåºï¼‰
            sorted_pages = sorted(crawled_pages, key=lambda x: x["last_crawled_at"])
            intervals = []

            for i in range(1, len(sorted_pages)):
                prev_time = sorted_pages[i-1]["last_crawled_at"]
                curr_time = sorted_pages[i]["last_crawled_at"]

                # è®¡ç®—æ—¶é—´é—´éš”ï¼ˆç§’ï¼‰
                from datetime import datetime
                if isinstance(prev_time, str):
                    prev_dt = datetime.fromisoformat(prev_time.replace('Z', '+00:00'))
                    curr_dt = datetime.fromisoformat(curr_time.replace('Z', '+00:00'))
                else:
                    prev_dt = prev_time
                    curr_dt = curr_time

                interval_seconds = (curr_dt - prev_dt).total_seconds()
                intervals.append(interval_seconds)

            # è®¡ç®—å¹³å‡å€¼ï¼ˆintervalsåœ¨æ­¤ä½œç”¨åŸŸå†…å·²å®šä¹‰ï¼‰
            if intervals:
                avg_crawl_interval = sum(intervals) / len(intervals)

        # æ ¼å¼åŒ–æ•°æ®
        formatted_pages = []
        for page in pages:
            # ç®€åŒ–URLæ˜¾ç¤º
            display_url = page["url"]
            if display_url.startswith("https://developer.apple.com/documentation"):
                display_url = display_url.replace("https://developer.apple.com/documentation", "...")

            formatted_pages.append({
                "id": page["id"],  # æ•°æ®åº“å±‚å·²å¤„ç†UUIDåºåˆ—åŒ–
                "url": display_url,
                "full_url": page["url"],  # å®Œæ•´URL
                "content": page["content"][:100] + "..." if len(page["content"]) > 100 else page["content"],
                "full_content": page["content"],  # å®Œæ•´å†…å®¹
                "crawl_count": page["crawl_count"],
                "process_count": page["process_count"],
                "created_at": page["created_at"],  # æ•°æ®åº“å±‚å·²è½¬æ¢ä¸ºISOæ ¼å¼
                "last_crawled_at": page["last_crawled_at"]  # æ•°æ®åº“å±‚å·²è½¬æ¢ä¸ºISOæ ¼å¼
            })

        return JSONResponse({
            "success": True,
            "data": formatted_pages,
            "count": len(formatted_pages),
            "stats": {
                "avg_crawl_interval": f"{avg_crawl_interval:.2f}" if avg_crawl_interval else None,
                "crawled_pages_count": len(crawled_pages) if 'crawled_pages' in locals() else 0
            }
        })
    except Exception as e:
        return JSONResponse({
            "success": False,
            "error": str(e),
            "data": []
        }, status_code=500)


@app.get("/api/chunks")
async def get_chunks(page: int = 1, size: int = 50, search: str = "",
                    page_id: str = "", sort: str = "created_at", order: str = "desc") -> JSONResponse:
    """è·å–chunksè¡¨æ•°æ®ï¼ˆåˆ†é¡µï¼‰"""
    try:
        client = await get_database_client()

        # æ„å»ºæŸ¥è¯¢æ¡ä»¶
        where_conditions = []
        params = []

        if search:
            where_conditions.append("(url ILIKE $1 OR content ILIKE $1)")
            params.append(f"%{search}%")

        if page_id:
            param_index = len(params) + 1
            where_conditions.append(f"url IN (SELECT url FROM pages WHERE id = ${param_index}::uuid)")
            params.append(page_id)

        where_clause = "WHERE " + " AND ".join(where_conditions) if where_conditions else ""

        # æ„å»ºæ’åº
        valid_sorts = ["id", "url", "created_at"]
        sort_column = sort if sort in valid_sorts else "created_at"
        sort_order = "ASC" if order.lower() == "asc" else "DESC"

        # è®¡ç®—åˆ†é¡µ
        offset = (page - 1) * size

        # è·å–æ€»æ•°
        count_query = f"SELECT COUNT(*) as total FROM chunks {where_clause}"
        total_result = await client.fetch_all(count_query, *params)
        total = total_result[0]["total"]

        # è·å–åˆ†é¡µæ•°æ®
        limit_param = len(params) + 1
        offset_param = len(params) + 2
        query = f"""
            SELECT id, url, content, created_at, embedding
            FROM chunks {where_clause}
            ORDER BY {sort_column} {sort_order}
            LIMIT ${limit_param} OFFSET ${offset_param}
        """
        params.extend([size, offset])
        chunks = await client.fetch_all(query, *params)

        # æ ¼å¼åŒ–æ•°æ®
        formatted_chunks = []
        for chunk in chunks:
                # ç®€åŒ–URLæ˜¾ç¤º
                display_url = chunk["url"]
                if display_url.startswith("https://developer.apple.com/documentation"):
                    display_url = display_url.replace("https://developer.apple.com/documentation", "...")

                # å¤„ç†embeddingæ•°æ® - ç›´æ¥æ˜¾ç¤ºå‰5ä¸ªå€¼
                embedding_info = "æ— "
                if chunk["embedding"]:
                    try:
                        import ast
                        embedding_array = ast.literal_eval(chunk["embedding"])
                        if isinstance(embedding_array, list) and len(embedding_array) > 0:
                            # ç›´æ¥æ˜¾ç¤ºå‰5ä¸ªå€¼ï¼Œä¿ç•™4ä½å°æ•°
                            embedding_info = str([round(x, 4) for x in embedding_array[:5]])
                    except Exception:
                        embedding_info = "è§£æé”™è¯¯"

                formatted_chunks.append({
                    "id": chunk["id"],  # æ•°æ®åº“å±‚å·²å¤„ç†UUIDåºåˆ—åŒ–
                    "url": display_url,
                    "full_url": chunk["url"],  # å®Œæ•´URL
                    "content": chunk["content"][:100] + "..." if len(chunk["content"]) > 100 else chunk["content"],
                    "full_content": chunk["content"],  # å®Œæ•´å†…å®¹
                    "created_at": chunk["created_at"],  # æ•°æ®åº“å±‚å·²è½¬æ¢ä¸ºISOæ ¼å¼
                    "embedding_info": embedding_info,
                    "raw_embedding": str(chunk["embedding"]) if chunk["embedding"] else None
                })

        return JSONResponse({
            "success": True,
            "data": formatted_chunks,
            "pagination": {
                "page": page,  # FastAPIå·²ç¡®ä¿æ˜¯æ•´æ•°
                "size": size,  # FastAPIå·²ç¡®ä¿æ˜¯æ•´æ•°
                "total": total,
                "pages": (total + size - 1) // size
            }
        })
    except Exception as e:
        return JSONResponse({
            "success": False,
            "error": str(e),
            "data": []
        }, status_code=500)


@app.get("/api/stats")
async def get_stats() -> JSONResponse:
    """è·å–ç»Ÿè®¡ä¿¡æ¯"""
    try:
        client = await get_database_client()

        # è·å–pagesåŸºç¡€ç»Ÿè®¡
        pages_count = await client.fetch_all("SELECT COUNT(*) as count FROM pages")
        chunks_count = await client.fetch_all("SELECT COUNT(*) as count FROM chunks")

        # è·å–æœ‰contentçš„pagesç»Ÿè®¡
        pages_with_content = await client.fetch_all("""
            SELECT COUNT(*) as count FROM pages
            WHERE content IS NOT NULL AND content != ''
        """)

        # è·å–å¹³å‡crawl countå’Œprocess count
        avg_crawl_count = await client.fetch_all("""
            SELECT AVG(crawl_count) as avg_count FROM pages
        """)

        avg_process_count = await client.fetch_all("""
            SELECT AVG(process_count) as avg_count FROM pages
            WHERE content IS NOT NULL AND content != ''
        """)

        # è·å–å¼‚å¸¸é¡µé¢ç»Ÿè®¡ï¼ˆå·²çˆ¬å–ä½†å†…å®¹å¼‚å¸¸çŸ­ï¼‰
        anomalous_pages = await client.fetch_all("""
            SELECT COUNT(*) as count FROM pages
            WHERE crawl_count > 0 AND LENGTH(content) < 10
        """)

        total_pages = pages_count[0]["count"]
        content_pages = pages_with_content[0]["count"]
        content_percentage = (content_pages / total_pages * 100) if total_pages > 0 else 0
        avg_crawl = float(avg_crawl_count[0]["avg_count"]) if avg_crawl_count[0]["avg_count"] else 0
        avg_process = float(avg_process_count[0]["avg_count"]) if avg_process_count[0]["avg_count"] else 0

        return JSONResponse({
            "success": True,
            "data": {
                "pages_count": total_pages,
                "chunks_count": chunks_count[0]["count"],
                "pages_with_content": content_pages,
                "content_percentage": f"{content_percentage:.2f}",
                "avg_crawl_count": round(avg_crawl, 4),
                "avg_process_count": round(avg_process, 4),
                "anomalous_pages": anomalous_pages[0]["count"]
            }
        })
    except Exception as e:
        return JSONResponse({
            "success": False,
            "error": str(e),
            "data": {
                "pages_count": 0,
                "chunks_count": 0,
                "pages_with_content": 0,
                "content_percentage": 0,
                "avg_crawl_count": 0,
                "avg_process_count": 0
            }
        }, status_code=500)


@app.get("/")
async def root():
    """æ ¹è·¯å¾„"""
    return {"message": "Database Viewer API", "version": "1.0.0"}


if __name__ == "__main__":
    print("ğŸš€ å¯åŠ¨æ•°æ®åº“æŸ¥çœ‹å™¨API...")
    print("ğŸ“Š APIåœ°å€: http://localhost:8001")
    print("ğŸ“– APIæ–‡æ¡£: http://localhost:8001/docs")
    
    uvicorn.run(
        "api:app",
        host="0.0.0.0",
        port=8001,
        reload=True,
        log_level="info"
    )
